{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanozzz/QuantBacktests/blob/main/MarketBreadthAnalyzer_(TV_Data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9WQLEHm-gau",
        "outputId": "79b9fb61-9805-489d-b6f9-e9fa4a74a69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-01-03 13:57:59] Starting data download...\n",
            "[2025-01-03 13:57:59] Starting market analysis...\n",
            "Loaded symbols file: /content/stock_list_25feb.csv with 1260 rows.\n",
            "Loaded data from pickle file: /content/TV_Daily_Jan_03_2025.pkl with 917208 rows.\n",
            "Data and symbols loaded successfully.\n",
            "[2025-01-03 13:57:59] Starting Market Breadth metrics calculation.\n",
            "[2025-01-03 13:58:09] Market Breadth metrics calculated for 1181 days.\n",
            "Market Breadth report saved to /content/MarketBreadth_Jan_03_2025.csv.\n",
            "[2025-01-03 13:58:09] Generating charts...\n",
            "Plot saved as /content/25CR+_MA14_chart.png\n",
            "Plot saved as /content/50DMA_chart.png\n",
            "Plot saved as /content/100CR+_MA14_chart.png\n",
            "Plot saved as /content/200DMA_chart.png\n",
            "Plot saved as /content/50CR+_MA14_chart.png\n",
            "Bar plot saved as /content/4PCT_UP_Bar.png\n",
            "Bar plot saved as /content/10PCT_UP_Bar.png\n",
            "Bar plot saved as /content/4PCT_DOWN_Bar.png\n",
            "Bar plot saved as /content/ADRatio_Last20.png\n",
            "[2025-01-03 13:58:14] Sending updates to Telegram...\n",
            "[2025-01-03 13:58:14] Pipeline completed successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "from datetime import date, datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import pytz\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "import telegram\n",
        "from telegram.error import TelegramError\n",
        "from datetime import date\n",
        "import datetime\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "from telegram.error import TelegramError\n",
        "\n",
        "def log_with_timestamp(message):\n",
        "    ist = pytz.timezone(\"Asia/Kolkata\")\n",
        "    # Use datetime.datetime.now(...) since we imported the module\n",
        "    timestamp = datetime.datetime.now(ist).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "\n",
        "class DataDownloader:\n",
        "    def __init__(self, username, password, symbol_file, output_dir, max_workers=5):\n",
        "        self.username = username\n",
        "        self.password = password\n",
        "        self.symbol_file = symbol_file\n",
        "        self.output_dir = output_dir\n",
        "        self.tv = TvDatafeed(username, password)\n",
        "        self.symbols = None\n",
        "        self.data = []\n",
        "        self.failed_symbols = []\n",
        "        self.max_workers = max_workers\n",
        "\n",
        "    def load_symbols(self):\n",
        "        \"\"\"Load symbols from the given CSV file.\"\"\"\n",
        "        self.symbols = pd.read_csv(self.symbol_file)\n",
        "        log_with_timestamp(f\"Loaded {len(self.symbols)} symbols for download.\")\n",
        "\n",
        "    def _download_symbol_data(self, symbol, n_bars, max_retries=5):\n",
        "        \"\"\"Download data for a single symbol with retry and exponential backoff.\"\"\"\n",
        "        retries = 0\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                ts = self.tv.get_hist(symbol=symbol, interval=Interval.in_daily, n_bars=n_bars)\n",
        "                log_with_timestamp(f\"Downloaded: {symbol}\")\n",
        "                return ts, None\n",
        "            except Exception as e:\n",
        "                if \"429 Too Many Requests\" in str(e):\n",
        "                    wait_time = 2 ** retries  # Exponential backoff\n",
        "                    log_with_timestamp(f\"Rate limit hit for {symbol}. Retrying in {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    retries += 1\n",
        "                else:\n",
        "                    log_with_timestamp(f\"Error downloading {symbol}: {e}\")\n",
        "                    return None, e\n",
        "        log_with_timestamp(f\"Max retries reached for {symbol}.\")\n",
        "        return None, \"Max retries reached\"\n",
        "\n",
        "    def download_data(self, n_bars=800):\n",
        "        \"\"\"Download data concurrently for all symbols with rate limit handling.\"\"\"\n",
        "        log_with_timestamp(f\"Starting data download for {len(self.symbols)} symbols...\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            futures = {\n",
        "                executor.submit(self._download_symbol_data, symbol, n_bars): symbol\n",
        "                for symbol in self.symbols['Symbol']\n",
        "            }\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                symbol = futures[future]\n",
        "                try:\n",
        "                    ts, error = future.result()\n",
        "                    if ts is not None:\n",
        "                        self.data.append(ts)\n",
        "                    if error:\n",
        "                        self.failed_symbols.append(symbol)\n",
        "                except Exception as e:\n",
        "                    self.failed_symbols.append(symbol)\n",
        "                    log_with_timestamp(f\"Unexpected error for {symbol}: {e}\")\n",
        "\n",
        "        log_with_timestamp(f\"Download completed. {len(self.data)} symbols downloaded successfully.\")\n",
        "        log_with_timestamp(f\"{len(self.failed_symbols)} symbols failed to download.\")\n",
        "\n",
        "    def retry_failed_downloads(self, n_bars=210):\n",
        "        \"\"\"Retry downloads for failed symbols.\"\"\"\n",
        "        if not self.failed_symbols:\n",
        "            log_with_timestamp(\"No failed symbols to retry.\")\n",
        "            return\n",
        "\n",
        "        log_with_timestamp(f\"Retrying download for {len(self.failed_symbols)} failed symbols...\")\n",
        "        retries = []\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            futures = {\n",
        "                executor.submit(self._download_symbol_data, symbol, n_bars): symbol\n",
        "                for symbol in self.failed_symbols\n",
        "            }\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                symbol = futures[future]\n",
        "                try:\n",
        "                    ts, error = future.result()\n",
        "                    if ts is not None:\n",
        "                        self.data.append(ts)\n",
        "                    if error:\n",
        "                        retries.append(symbol)\n",
        "                except Exception as e:\n",
        "                    retries.append(symbol)\n",
        "                    log_with_timestamp(f\"Unexpected error for {symbol}: {e}\")\n",
        "\n",
        "        self.failed_symbols = retries\n",
        "        log_with_timestamp(f\"Retry completed. {len(self.failed_symbols)} symbols still failed.\")\n",
        "\n",
        "    def save_data(self):\n",
        "        \"\"\"Save the downloaded data as both a pickle file and a CSV file.\"\"\"\n",
        "        today = date.today().strftime(\"%b_%d_%Y\")\n",
        "        pickle_path = os.path.join(self.output_dir, f\"TV_Daily_{today}.pkl\")\n",
        "        csv_path = os.path.join(self.output_dir, f\"TV_Daily_{today}.csv\")\n",
        "\n",
        "        if self.data:\n",
        "            combined_data = pd.concat(self.data).reset_index(drop=False)\n",
        "            combined_data = combined_data.rename(\n",
        "                columns={\n",
        "                    'datetime': 'DateTime', 'symbol': 'Symbol',\n",
        "                    'open': 'Open', 'high': 'High',\n",
        "                    'low': 'Low', 'close': 'Close',\n",
        "                    'volume': 'Volume'\n",
        "                }\n",
        "            )\n",
        "            # Save as pickle\n",
        "            combined_data.to_pickle(pickle_path)\n",
        "            log_with_timestamp(f\"Data saved to pickle file: {pickle_path}\")\n",
        "\n",
        "            # Save as CSV\n",
        "            combined_data.to_csv(csv_path, index=False)\n",
        "            log_with_timestamp(f\"Data saved to CSV file: {csv_path}\")\n",
        "        else:\n",
        "            log_with_timestamp(\"No data to save.\")\n",
        "\n",
        "    def download_and_save(self):\n",
        "        \"\"\"Full workflow: load symbols, download data, retry failed, and save.\"\"\"\n",
        "        self.load_symbols()\n",
        "        self.download_data()\n",
        "        self.retry_failed_downloads()\n",
        "        self.save_data()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MarketAnalysis:\n",
        "    def __init__(self, input_dir, output_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.symbols = None\n",
        "        self.data = None\n",
        "        self.results = None\n",
        "\n",
        "    def load_data(self, symbol_file, data_file):\n",
        "        \"\"\"Load symbol and data files.\"\"\"\n",
        "        try:\n",
        "            # Load the symbols file (always CSV)\n",
        "            self.symbols = pd.read_csv(os.path.join(self.input_dir, symbol_file))\n",
        "            log_with_timestamp(f\"Loaded symbols file: {symbol_file} with {len(self.symbols)} rows.\")\n",
        "\n",
        "            # Determine the data file format based on the extension\n",
        "            data_path = os.path.join(self.input_dir, data_file)\n",
        "            if data_file.endswith('.pkl'):\n",
        "                self.data = pd.read_pickle(data_path)\n",
        "                log_with_timestamp(f\"Loaded data from pickle file: {data_file} with {len(self.data)} rows.\")\n",
        "            elif data_file.endswith('.csv'):\n",
        "                self.data = pd.read_csv(data_path)\n",
        "                log_with_timestamp(f\"Loaded data from CSV file: {data_file} with {len(self.data)} rows.\")\n",
        "            else:\n",
        "                log_with_timestamp(f\"Unsupported file format: {data_file}\")\n",
        "                raise ValueError(\"Data file must be a .pkl or .csv file\")\n",
        "\n",
        "            log_with_timestamp(\"Data and symbols loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            log_with_timestamp(f\"Error loading data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def filter_symbols(self, filter_key=\"FnO\", filter_value=0):\n",
        "        \"\"\"Filter symbols based on a specific key and value.\"\"\"\n",
        "        try:\n",
        "            self.symbols = self.symbols[self.symbols[filter_key] == filter_value]\n",
        "            self.data = self.data[self.data['Symbol'].isin(self.symbols['Symbol'])]\n",
        "            log_with_timestamp(\"Filtered symbols and data.\")\n",
        "        except Exception as e:\n",
        "            log_with_timestamp(f\"Error filtering symbols: {e}\")\n",
        "            raise\n",
        "\n",
        "class MarketAnalysis:\n",
        "    def __init__(self, input_dir, output_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.symbols = None\n",
        "        self.data = None\n",
        "        self.results = None\n",
        "\n",
        "    def load_data(self, symbol_file, data_file):\n",
        "        \"\"\"Load symbol and data files.\"\"\"\n",
        "        self.symbols = pd.read_csv(os.path.join(self.input_dir, symbol_file))\n",
        "        print(f\"Loaded symbols file: {symbol_file} with {len(self.symbols)} rows.\")\n",
        "\n",
        "        data_path = os.path.join(self.input_dir, data_file)\n",
        "        if data_file.endswith('.pkl'):\n",
        "            self.data = pd.read_pickle(data_path)\n",
        "            print(f\"Loaded data from pickle file: {data_file} with {len(self.data)} rows.\")\n",
        "        elif data_file.endswith('.csv'):\n",
        "            self.data = pd.read_csv(data_path)\n",
        "            print(f\"Loaded data from CSV file: {data_file} with {len(self.data)} rows.\")\n",
        "        else:\n",
        "            raise ValueError(\"Data file must be a .pkl or .csv file\")\n",
        "\n",
        "        print(\"Data and symbols loaded successfully.\")\n",
        "\n",
        "    def process_data(self):\n",
        "        \"\"\"\n",
        "        1) Sort data by Symbol and DateTime.\n",
        "        2) Compute 50DMA and 200DMA for each symbol.\n",
        "        3) Group by daily date and compute market breadth metrics.\n",
        "        4) Store the final DataFrame in self.results.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            log_with_timestamp(\"Starting Market Breadth metrics calculation.\")\n",
        "\n",
        "            # ----------------------------\n",
        "            #  STEP 1: Sort data\n",
        "            # ----------------------------\n",
        "            # Ensure data is in chronological order for each symbol\n",
        "            self.data.sort_values(by=['Symbol', 'DateTime'], inplace=True)\n",
        "\n",
        "            # ----------------------------\n",
        "            #  STEP 2: Compute 50DMA & 200DMA\n",
        "            # ----------------------------\n",
        "            # For each symbol, compute rolling average over the Close price\n",
        "            self.data['50DMA'] = self.data.groupby('Symbol')['Close'].transform(\n",
        "                lambda x: x.rolling(window=50, min_periods=1).mean()\n",
        "            )\n",
        "            self.data['200DMA'] = self.data.groupby('Symbol')['Close'].transform(\n",
        "                lambda x: x.rolling(window=200, min_periods=1).mean()\n",
        "            )\n",
        "\n",
        "            # ----------------------------\n",
        "            #  STEP 3: Group by date\n",
        "            # ----------------------------\n",
        "            # Convert DateTime to just a date (year-month-day)\n",
        "            self.data['Date'] = pd.to_datetime(self.data['DateTime']).dt.date\n",
        "            daily_data = self.data.groupby('Date')\n",
        "\n",
        "            metrics_list = []\n",
        "\n",
        "            # ----------------------------\n",
        "            #  STEP 4: Compute daily metrics\n",
        "            # ----------------------------\n",
        "            for current_date, group in daily_data:\n",
        "                total = len(group)\n",
        "                # 4%, 10%, 15% up\n",
        "                pct_4_up = len(group[group['Close'] >= group['Open'] * 1.04])\n",
        "                pct_10_up = len(group[group['Close'] >= group['Open'] * 1.10])\n",
        "                pct_15_up = len(group[group['Close'] >= group['Open'] * 1.15])\n",
        "                # 4%, 10%, 15% down\n",
        "                pct_4_down = len(group[group['Close'] <= group['Open'] * 0.96])\n",
        "                pct_10_down = len(group[group['Close'] <= group['Open'] * 0.90])\n",
        "                pct_15_down = len(group[group['Close'] <= group['Open'] * 0.85])\n",
        "\n",
        "                # Percentage of stocks above their 200DMA and 50DMA\n",
        "                pct_above_200dma = 0\n",
        "                pct_above_50dma = 0\n",
        "                if total > 0:\n",
        "                    pct_above_200dma = len(group[group['Close'] > group['200DMA']]) / total * 100\n",
        "                    pct_above_50dma = len(group[group['Close'] > group['50DMA']]) / total * 100\n",
        "\n",
        "                # 25CR+, 50CR+, 100CR+ as counts (number of symbols exceeding each threshold)\n",
        "                count_25cr = len(group[group['Volume'] >= 25e6])\n",
        "                count_50cr = len(group[group['Volume'] >= 50e6])\n",
        "                count_100cr = len(group[group['Volume'] >= 100e6])\n",
        "\n",
        "                # A/D ratio\n",
        "                ad_ratio = (pct_4_up + pct_10_up + pct_15_up) / (pct_4_down + pct_10_down + pct_15_down + 1e-6)\n",
        "\n",
        "                # Collect in metrics_list\n",
        "                metrics_list.append({\n",
        "                    \"Date\": current_date,\n",
        "                    \"Total\": total,\n",
        "                    \"4PCT_UP\": pct_4_up,\n",
        "                    \"10PCT_UP\": pct_10_up,\n",
        "                    \"15PCT_UP\": pct_15_up,\n",
        "                    \"4PCT_DOWN\": pct_4_down,\n",
        "                    \"10PCT_DOWN\": pct_10_down,\n",
        "                    \"15PCT_DOWN\": pct_15_down,\n",
        "                    \"50DMA\": pct_above_50dma,    # % of stocks above their 50-day MA\n",
        "                    \"200DMA\": pct_above_200dma, # % of stocks above their 200-day MA\n",
        "                    \"25CR+\": count_25cr,        # count of stocks with Volume >= 25e6\n",
        "                    \"50CR+\": count_50cr,        # count of stocks with Volume >= 50e6\n",
        "                    \"100CR+\": count_100cr,      # count of stocks with Volume >= 100e6\n",
        "                    \"ADRatio\": ad_ratio\n",
        "                })\n",
        "\n",
        "            # ----------------------------\n",
        "            #  STEP 5: Create final DataFrame\n",
        "            # ----------------------------\n",
        "            self.results = pd.DataFrame(metrics_list)\n",
        "            if self.results.empty:\n",
        "                log_with_timestamp(\"No daily metrics computed (empty results).\")\n",
        "                return\n",
        "\n",
        "            # Example rolling columns on the daily aggregates:\n",
        "            rolling_cols = [\n",
        "                '200DMA', '50DMA',\n",
        "                '4PCT_UP', '4PCT_DOWN',\n",
        "                '10PCT_UP', '10PCT_DOWN',\n",
        "                '25CR+', '50CR+', '100CR+'\n",
        "            ]\n",
        "\n",
        "            # Sort self.results by Date just in case\n",
        "            self.results.sort_values(by='Date', inplace=True)\n",
        "\n",
        "            for col in rolling_cols:\n",
        "                if col in self.results.columns:\n",
        "                    self.results[f\"{col}_MA14\"] = self.results[col].rolling(window=14).mean()\n",
        "                else:\n",
        "                    log_with_timestamp(f\"Column '{col}' not in results. Skipping rolling average.\")\n",
        "\n",
        "            # Rolling A/D ratio\n",
        "            if 'ADRatio' in self.results.columns:\n",
        "                self.results['A/D_7MA'] = self.results['ADRatio'].rolling(window=7).mean()\n",
        "\n",
        "            log_with_timestamp(f\"Market Breadth metrics calculated for {len(self.results)} days.\")\n",
        "        except Exception as e:\n",
        "            log_with_timestamp(f\"Error processing data: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def save_market_breadth_report(self):\n",
        "        \"\"\"Save the Market Breadth report.\"\"\"\n",
        "        today = date.today().strftime(\"%b_%d_%Y\")\n",
        "        file_path = os.path.join(self.output_dir, f\"MarketBreadth_{today}.csv\")\n",
        "        if not self.results.empty:\n",
        "            self.results.to_csv(file_path, index=False)\n",
        "            print(f\"Market Breadth report saved to {file_path}.\")\n",
        "        else:\n",
        "            print(\"No Market Breadth metrics to save.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "class MarketBreadthPlot:\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Initialize the MarketBreadthPlot class.\n",
        "\n",
        "        Parameters:\n",
        "        - data (pd.DataFrame): DataFrame containing market breadth data with Date as a column.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.data['Date'] = pd.to_datetime(self.data['Date'])  # Ensure 'Date' is in datetime format\n",
        "\n",
        "    def plot_metric(self, x_col='Date', y_col=None, ma_col=None, title=None, ylabel=None, save_path=None):\n",
        "        \"\"\"\n",
        "        Plot a metric (y_col) and/or a moving average (ma_col) against x_col.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        if y_col:\n",
        "            plt.plot(self.data[x_col], self.data[y_col], label=y_col, linewidth=1.5)\n",
        "        if ma_col:\n",
        "            plt.plot(self.data[x_col], self.data[ma_col], label=ma_col, color='red', linewidth=1.5)\n",
        "\n",
        "        # Customize plot\n",
        "        plt.title(title, fontsize=16)\n",
        "        plt.ylabel(ylabel, fontsize=14)\n",
        "        plt.xlabel('Date', fontsize=14)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, bbox_inches='tight', format='png')\n",
        "            print(f\"Plot saved as {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def plot_bar_metric(self, x_col='Date', y_col=None, title=None, ylabel=None, save_path=None, last_days=None):\n",
        "        \"\"\"\n",
        "        Plot a bar chart for a given metric (y_col) and restrict to the last N days if specified.\n",
        "        \"\"\"\n",
        "        data = self.data\n",
        "        if last_days:\n",
        "            cutoff_date = data[x_col].max() - timedelta(days=last_days)\n",
        "            data = data[data[x_col] >= cutoff_date]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.bar(data[x_col], data[y_col], label=y_col, alpha=0.7)\n",
        "\n",
        "        # Customize plot\n",
        "        plt.title(title, fontsize=16)\n",
        "        plt.ylabel(ylabel, fontsize=14)\n",
        "        plt.xlabel('Date', fontsize=14)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, bbox_inches='tight', format='png')\n",
        "            print(f\"Bar plot saved as {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def plot_all(self, save_dir=None, from_2022_only=True):\n",
        "        \"\"\"\n",
        "        Generate and save all the required charts for market breadth data.\n",
        "\n",
        "        Parameters:\n",
        "        - save_dir (str): Directory to save the plots (optional).\n",
        "        - from_2022_only (bool): If True, filters data to only show dates from 2022-01-01 onward.\n",
        "        \"\"\"\n",
        "        if from_2022_only:\n",
        "            self.data = self.data[self.data[\"Date\"] >= pd.to_datetime(\"2022-01-01\")]\n",
        "\n",
        "        plots = [\n",
        "            {\"ma_col\": \"25CR+_MA14\", \"title\": \"No. of stocks with turnover 25cr+ (7 Day Avg)\", \"ylabel\": \"25CR+\", \"save_name\": \"25CR+_MA14_chart.png\"},\n",
        "            {\"y_col\": \"50DMA\", \"ma_col\": \"50DMA_MA14\", \"title\": \"% of Stocks Above 50DMA\", \"ylabel\": \"50DMA\", \"save_name\": \"50DMA_chart.png\"},\n",
        "            {\"ma_col\": \"100CR+_MA14\", \"title\": \"No. of stocks with turnover 100cr+ (7 Day Avg)\", \"ylabel\": \"100CR+\", \"save_name\": \"100CR+_MA14_chart.png\"},\n",
        "            {\"y_col\": \"200DMA\", \"ma_col\": \"200DMA_MA14\", \"title\": \"% of Stocks Above 200DMA\", \"ylabel\": \"200DMA\", \"save_name\": \"200DMA_chart.png\"},\n",
        "            {\"ma_col\": \"50CR+_MA14\", \"title\": \"No. of stocks with turnover 50cr+ (7 Day Avg)\", \"ylabel\": \"50CR+\", \"save_name\": \"50CR+_MA14_chart.png\"}\n",
        "        ]\n",
        "\n",
        "        for plot in plots:\n",
        "            save_path = None\n",
        "            if save_dir:\n",
        "                save_path = os.path.join(save_dir, plot[\"save_name\"])\n",
        "            self.plot_metric(\n",
        "                x_col=\"Date\",\n",
        "                y_col=plot.get(\"y_col\"),\n",
        "                ma_col=plot.get(\"ma_col\"),\n",
        "                title=plot[\"title\"],\n",
        "                ylabel=plot[\"ylabel\"],\n",
        "                save_path=save_path\n",
        "            )\n",
        "\n",
        "    def plot_additional_metrics(self, save_dir=None):\n",
        "        \"\"\"\n",
        "        Generate bar plots for 4% UP, 10% UP, 4% DOWN, and ADRatio for the last 60 days.\n",
        "        \"\"\"\n",
        "        plots = [\n",
        "            {\"y_col\": \"4PCT_UP\", \"title\": \"4% UP (Last 60 Days)\", \"ylabel\": \"No. of Stocks\", \"save_name\": \"4PCT_UP_Bar.png\"},\n",
        "            {\"y_col\": \"10PCT_UP\", \"title\": \"10% UP (Last 60 Days)\", \"ylabel\": \"No. of Stocks\", \"save_name\": \"10PCT_UP_Bar.png\"},\n",
        "            {\"y_col\": \"4PCT_DOWN\", \"title\": \"4% DOWN (Last 60 Days)\", \"ylabel\": \"No. of Stocks\", \"save_name\": \"4PCT_DOWN_Bar.png\"},\n",
        "            {\"y_col\": \"ADRatio\", \"title\": \"ADRatio (Last 60 Days)\", \"ylabel\": \"AD Ratio\", \"save_name\": \"ADRatio_Last20.png\"}\n",
        "        ]\n",
        "\n",
        "        for plot in plots:\n",
        "            save_path = None\n",
        "            if save_dir:\n",
        "                save_path = os.path.join(save_dir, plot[\"save_name\"])\n",
        "            self.plot_bar_metric(\n",
        "                x_col=\"Date\",\n",
        "                y_col=plot[\"y_col\"],\n",
        "                title=plot[\"title\"],\n",
        "                ylabel=plot[\"ylabel\"],\n",
        "                save_path=save_path,\n",
        "                last_days=60\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "class TelegramChartSender:\n",
        "    \"\"\"\n",
        "    Encapsulates the logic for sending text messages and chart images to a Telegram chat or channel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bot_token: str, chat_id: str):\n",
        "        \"\"\"\n",
        "        Initializes the Telegram bot with the given token and sets the chat/channel ID.\n",
        "\n",
        "        Args:\n",
        "            bot_token (str): Token provided by BotFather, e.g. \"123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11\"\n",
        "            chat_id (str or int): Numeric chat ID (e.g., -1001234567890 for groups) or channel username (e.g., \"@my_channel\")\n",
        "        \"\"\"\n",
        "        self.bot_token = bot_token\n",
        "        self.chat_id = chat_id\n",
        "        self.bot = telegram.Bot(token=self.bot_token)\n",
        "\n",
        "    def send_text_update(self, custom_text: str = None):\n",
        "        \"\"\"\n",
        "        Sends a text message to the configured Telegram chat/channel.\n",
        "        If no custom_text is provided, sends a default \"Here is today's update @ <timestamp>\" message.\n",
        "\n",
        "        Args:\n",
        "            custom_text (str): Optional text to send. If None, uses a default with a timestamp.\n",
        "        \"\"\"\n",
        "        if not custom_text:\n",
        "            # Generate a default message with timestamp\n",
        "            timestamp_str = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            custom_text = f\"Here is today's update @ {timestamp_str}\"\n",
        "\n",
        "        try:\n",
        "            self.bot.send_message(chat_id=self.chat_id, text=custom_text)\n",
        "            print(f\"Sent text message: {custom_text}\")\n",
        "        except TelegramError as e:\n",
        "            print(f\"Error sending text message: {e}\")\n",
        "\n",
        "    def send_charts(self, file_paths: list):\n",
        "        \"\"\"\n",
        "        Sends a list of .png (or other image) files to the configured Telegram chat/channel.\n",
        "\n",
        "        Args:\n",
        "            file_paths (list): List of file paths (strings) to .png images.\n",
        "        \"\"\"\n",
        "        for file_path in file_paths:\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    with open(file_path, 'rb') as chart_file:\n",
        "                        # Send as a photo\n",
        "                        self.bot.send_photo(chat_id=self.chat_id, photo=chart_file)\n",
        "                    print(f\"Sent chart: {file_path}\")\n",
        "                except TelegramError as e:\n",
        "                    print(f\"Error sending {file_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"File not found: {file_path}\")\n",
        "\n",
        "    def send_daily_update(self, file_paths: list, custom_text: str = None):\n",
        "        \"\"\"\n",
        "        Sends a text update (with a timestamp or custom text), then sends any charts provided.\n",
        "\n",
        "        Args:\n",
        "            file_paths (list): List of file paths (strings) to .png images.\n",
        "            custom_text (str): Optional message text to send before charts.\n",
        "        \"\"\"\n",
        "        # First send a text update (timestamp or custom)\n",
        "        self.send_text_update(custom_text=custom_text)\n",
        "        # Then send the charts\n",
        "        self.send_charts(file_paths)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MarketBreadthPipeline:\n",
        "    \"\"\"\n",
        "    Encapsulates the entire pipeline for market breadth analysis,\n",
        "    including data download, metric calculation, plotting, and Telegram updates.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bot_token, group_id):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline.\n",
        "\n",
        "        Args:\n",
        "            bot_token (str): Telegram bot token.\n",
        "            group_id (str): Telegram group/chat ID.\n",
        "        \"\"\"\n",
        "        # Constants\n",
        "        self.username = 'nileshiit'  # Username for data download\n",
        "        self.password = 'Hari@123om'  # Password for data download\n",
        "        self.symbol_file = '/content/stock_list_25feb.csv'  # Symbol file path\n",
        "        self.output_dir = '/content'  # Directory for saving data and charts\n",
        "\n",
        "        # Telegram Details\n",
        "        self.bot_token = bot_token\n",
        "        self.group_id = group_id\n",
        "\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Execute the full pipeline: download data, analyze, plot, and send Telegram updates.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Step 1: Data Download (Optional)\n",
        "            log_with_timestamp(\"Starting data download...\")\n",
        "            downloader = DataDownloader(self.username, self.password, self.symbol_file, self.output_dir, max_workers=5)\n",
        "            downloader.download_and_save()\n",
        "\n",
        "            # Step 2: Market Analysis\n",
        "            log_with_timestamp(\"Starting market analysis...\")\n",
        "            analysis = MarketAnalysis(self.output_dir, self.output_dir)\n",
        "            today = date.today().strftime(\"%b_%d_%Y\")\n",
        "            data_file = f\"{self.output_dir}/TV_Daily_{today}.pkl\"\n",
        "            analysis.load_data(self.symbol_file, data_file)\n",
        "            analysis.process_data()\n",
        "            analysis.save_market_breadth_report()\n",
        "\n",
        "            # Step 3: Generate Charts\n",
        "            log_with_timestamp(\"Generating charts...\")\n",
        "            csv_file_path = os.path.join(self.output_dir, f\"MarketBreadth_{today}.csv\")\n",
        "            market_data = pd.read_csv(csv_file_path)\n",
        "\n",
        "            # Create plots\n",
        "            plotter = MarketBreadthPlot(market_data)\n",
        "            plotter.plot_all(save_dir=self.output_dir)\n",
        "            plotter.plot_additional_metrics(save_dir=self.output_dir)\n",
        "\n",
        "            # Step 4: Send Updates to Telegram\n",
        "            log_with_timestamp(\"Sending updates to Telegram...\")\n",
        "            chart_paths = [\n",
        "                os.path.join(self.output_dir, \"25CR+_MA14_chart.png\"),\n",
        "                os.path.join(self.output_dir, \"50DMA_chart.png\"),\n",
        "                os.path.join(self.output_dir, \"100CR+_MA14_chart.png\"),\n",
        "                os.path.join(self.output_dir, \"200DMA_chart.png\"),\n",
        "                os.path.join(self.output_dir, \"4PCT_UP_Bar.png\"),\n",
        "                os.path.join(self.output_dir, \"10PCT_UP_Bar.png\"),\n",
        "                os.path.join(self.output_dir, \"4PCT_DOWN_Bar.png\"),\n",
        "                os.path.join(self.output_dir, \"ADRatio_Last20.png\"),\n",
        "            ]\n",
        "\n",
        "            custom_text = f\"Hello, group! Here is today's Market Breadth update ({date.today().strftime('%Y-%m-%d')})\"\n",
        "            sender = TelegramChartSender(bot_token=self.bot_token, chat_id=self.group_id)\n",
        "            sender.send_daily_update(file_paths=chart_paths, custom_text=custom_text)\n",
        "\n",
        "            log_with_timestamp(\"Pipeline completed successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log_with_timestamp(f\"Error during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # Retrieve environment variables\n",
        "    token = userdata.get('TELEGRAM_BOT_TOKEN')\n",
        "    group_id = userdata.get('TELEGRAM_GROUP_ID')\n",
        "\n",
        "    # Run the pipeline\n",
        "    pipeline = MarketBreadthPipeline(bot_token=token, group_id=group_id)\n",
        "    pipeline.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QzZ7Ikb-8h2"
      },
      "outputs": [],
      "source": [
        "#url = 'https://anaconda.org/conda-forge/libta-lib/0.4.0/download/linux-64/libta-lib-0.4.0-h166bdaf_1.tar.bz2'\n",
        "#!curl -L $url | tar xj -C /usr/lib/x86_64-linux-gnu/ lib --strip-components=1\n",
        "#url = 'https://anaconda.org/conda-forge/ta-lib/0.4.19/download/linux-64/ta-lib-0.4.19-py310hde88566_4.tar.bz2'\n",
        "#!curl -L $url | tar xj -C /usr/local/lib/python3.10/dist-packages/ lib/python3.10/site-packages/talib --strip-components=3\n",
        "#import talib\n",
        "\n",
        "#!pip install mypy_extensions schedule kiteconnect gspread_pandas dateparser\n",
        "#!pip install python-telegram-bot==13.14"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGphJhtoY/X5NixIUrLqY6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}