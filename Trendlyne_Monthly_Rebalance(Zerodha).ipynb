{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanozzz/QuantBacktests/blob/main/Trendlyne_Monthly_Rebalance(Zerodha).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6SB8YlhqrW0J"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "STEP0::: IMPORTING LIBRARIES AND SETTING UP GLOBAL VARIABLES\n",
        "\n",
        "\"\"\"\n",
        "import os\n",
        "import time\n",
        "from pytz import timezone\n",
        "import logging\n",
        "import re\n",
        "import sys\n",
        "sys.path.append('/content/vectorbt.pro')  # Add the path where your package is cloned\n",
        "# Now, try importing your module\n",
        "import vectorbtpro as vbt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pandas.tseries.offsets import MonthBegin\n",
        "import talib\n",
        "from numba import njit\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from pytz import timezone as tz\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from kiteconnect import KiteConnect\n",
        "\n",
        "# Global variable\n",
        "\n",
        "api_key = \"lwhfs47ilbvwc84g\"\n",
        "access_token = \"51V71sUSPlYdSI0EVypYy6cj84B3bsJX\"\n",
        "max_month = 100\n",
        "\n",
        "\n",
        "# Load the CSV file\n",
        "input_file = \"BacktestInput.csv\"  # Replace with your actual file path\n",
        "df = pd.read_csv(input_file, on_bad_lines='skip')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZCDrJXw1Q4X",
        "outputId": "2ff52b0d-7874-4923-defc-21a5c09fdf84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file saved to symbols.json\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "STEP0::: SETUP LOGGER\n",
        "\n",
        "\"\"\"\n",
        "# Define IST timezone\n",
        "IST = timezone('Asia/Kolkata')\n",
        "\n",
        "class ISTFormatter(logging.Formatter):\n",
        "    \"\"\"\n",
        "    Custom formatter to force log timestamps to use IST timezone.\n",
        "    \"\"\"\n",
        "    def formatTime(self, record, datefmt=None):\n",
        "        # Convert the record's created time to IST\n",
        "        record_time = datetime.fromtimestamp(record.created).astimezone(IST)\n",
        "        if datefmt:\n",
        "            return record_time.strftime(datefmt)\n",
        "        return record_time.isoformat()\n",
        "\n",
        "def setup_logger(log_file: str, level=logging.INFO, file_mode='a') -> logging.Logger:\n",
        "    \"\"\"\n",
        "    Sets up a logger with timestamps in IST for both file and console outputs.\n",
        "    Ensures no duplicate log handlers are added.\n",
        "\n",
        "    Args:\n",
        "        log_file (str): Path to the log file.\n",
        "        level (int): Logging level (e.g., logging.INFO, logging.DEBUG).\n",
        "        file_mode (str): Mode to open the log file ('a' for append, 'w' for overwrite).\n",
        "\n",
        "    Returns:\n",
        "        logging.Logger: Configured logger instance.\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(\"OrderBookGeneratorLogger\")\n",
        "    logger.setLevel(level)\n",
        "\n",
        "    # Clear existing handlers\n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    # Define the custom IST formatter\n",
        "    formatter = ISTFormatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # File handler\n",
        "    file_handler = logging.FileHandler(log_file, mode=file_mode)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    # Console handler\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "# Set the global timezone for the process (if on POSIX system)\n",
        "os.environ['TZ'] = 'Asia/Kolkata'\n",
        "try:\n",
        "    time.tzset()  # Apply timezone change globally (works on POSIX systems)\n",
        "except AttributeError:\n",
        "    # time.tzset() is not available on non-POSIX systems (e.g., Windows)\n",
        "    pass\n",
        "\n",
        "# Define the log file path\n",
        "log_file_path = f\"logfile_{datetime.now(IST).strftime('%Y-%m-%d')}.log\"\n",
        "logger = setup_logger(log_file_path, level=logging.INFO, file_mode='a')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "STEP1:::CODE FOR CONVERTING TRENDLYNE CSV TO JSON FILE\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Delete all rows with NaN values\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "# Step 2: Drop unnecessary columns\n",
        "columns_to_drop = [\n",
        "    \"Start Price\", \"End Price\", \"Change\",\n",
        "    \"Avg Change %\", \"Weightage of Each stock %\", \"NAV (Initial 100)\"\n",
        "]\n",
        "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Step 3: Forward fill the \"As on Date\" column for rows without a date\n",
        "df['As on Date'] = df['As on Date'].fillna(method='ffill')\n",
        "\n",
        "# Rename \"As on Date\" to \"Date\"\n",
        "df.rename(columns={\"As on Date\": \"Date\"}, inplace=True)\n",
        "\n",
        "# Step 4: Convert the date range (e.g., 2012-09-28 to 2012-10-31) to the first of the last date\n",
        "df['Date'] = df['Date'].str.extract(r'to\\s+(\\d{4}-\\d{2}-\\d{2})')  # Extract the last date\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce') - MonthBegin(1)  # Convert to first of the month\n",
        "\n",
        "# Step 5: Keep only rows where 'Stock' column is not blank\n",
        "df = df.dropna(subset=['Stock'])\n",
        "\n",
        "# Step 6: Extract the first part of the stock name and add 'NSE:' as prefix\n",
        "df['Stock'] = df['Stock'].str.extract(r'^([A-Z&-]+)').fillna('')  # Extract first part\n",
        "df['Stock'] = 'NSE:' + df['Stock']  # Add NSE prefix\n",
        "\n",
        "# Step 7: Convert the cleaned DataFrame to a JSON structure\n",
        "json_output = {}\n",
        "for _, row in df.iterrows():\n",
        "    date = row['Date'].strftime('%Y-%m-%d')  # Ensure date format\n",
        "    stock = row['Stock']\n",
        "    if date not in json_output:\n",
        "        json_output[date] = {}\n",
        "    json_output[date][stock] = 150000  # Assign default cash amount\n",
        "\n",
        "# Save the final JSON to symbols.json\n",
        "json_file = \"symbols.json\"\n",
        "with open(json_file, \"w\") as f:\n",
        "    json.dump(json_output, f, indent=4)\n",
        "\n",
        "print(f\"JSON file saved to {json_file}\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "STEP2:::ORDERBOOK GENERATOR CLASS FOR STGY\n",
        "\"\"\"\n",
        "\n",
        "# Define IST timezone\n",
        "IST = tz('Asia/Kolkata')\n",
        "\n",
        "class OrderBookGenerator:\n",
        "    def __init__(self, logger, api_key, access_token, log_file, symbols_json_path='/content/symbols.json', default_cash=50000,\n",
        "                tp_ladder=[0.2, 1], mult_factor=25, max_retries=5, retry_delay=2):\n",
        "        self.logger = logger\n",
        "        self.log_file = log_file\n",
        "        self.tp_ladder = tp_ladder\n",
        "        self.mult_factor = mult_factor\n",
        "        self.max_retries = max_retries\n",
        "        self.retry_delay = retry_delay\n",
        "        self.default_cash = default_cash\n",
        "\n",
        "        # Initialize KiteConnect\n",
        "        try:\n",
        "            self.kite = KiteConnect(api_key=api_key)\n",
        "            self.kite.set_access_token(access_token)\n",
        "            self.logger.info(\"KiteConnect initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error initializing KiteConnect: {e}\")\n",
        "            raise e\n",
        "\n",
        "        # Load symbols and their respective cash values\n",
        "        self.symbols_cash_map = self.load_symbols_cash(symbols_json_path)\n",
        "\n",
        "        # Determine the target month\n",
        "        self.target_month = self.get_target_month()\n",
        "\n",
        "        if self.target_month is None:\n",
        "            self.logger.warning(\"No valid target month found. Symbols list will be empty.\")\n",
        "\n",
        "        # Get symbols for the target month\n",
        "        self.symbols_list = self.get_symbols_for_target_month()\n",
        "\n",
        "        # Fetch instrument data from Kite API\n",
        "        self.instrument_df = self.fetch_instruments()\n",
        "\n",
        "        # Suppress warnings\n",
        "        warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def fetch_instruments(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch the list of all NSE instruments from KiteConnect API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Fetching NSE instruments from KiteConnect...\")\n",
        "            instrument_dump = self.kite.instruments(\"NSE\")\n",
        "            instrument_df = pd.DataFrame(instrument_dump)\n",
        "            instrument_df.to_csv(\"NSE_Instruments.csv\", index=False)  # Save as CSV for reference\n",
        "            self.logger.info(\"Fetched and saved NSE instrument dump.\")\n",
        "            return instrument_df\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching instruments: {e}\")\n",
        "            raise e\n",
        "\n",
        "\n",
        "\n",
        "    def load_symbols_cash(self, symbols_json_path: str) -> dict:\n",
        "        \"\"\"\n",
        "        Load the JSON file mapping months to symbols with cash values.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(symbols_json_path, 'r') as f:\n",
        "                symbols_cash_map = json.load(f)\n",
        "                self.logger.info(f\"Loaded symbols JSON from {symbols_json_path}\")\n",
        "                return symbols_cash_map\n",
        "        except FileNotFoundError:\n",
        "            self.logger.error(f\"Symbols JSON file not found: {symbols_json_path}. Exiting.\")\n",
        "            raise FileNotFoundError(f\"Symbols JSON file not found: {symbols_json_path}\")\n",
        "\n",
        "\n",
        "\n",
        "    def instrument_lookup(self, symbol: str) -> int:\n",
        "        \"\"\"\n",
        "        Looks up the instrument token for a given symbol from the instrument dump.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return self.instrument_df.loc[self.instrument_df['tradingsymbol'] == symbol, 'instrument_token'].values[0]\n",
        "        except IndexError:\n",
        "            self.logger.warning(f\"Instrument token not found for symbol: {symbol}.\")\n",
        "            return -1\n",
        "\n",
        "\n",
        "    def fetch_ohlc_extended(self, ticker: str, from_date: str, interval: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch historical data from Zerodha's KiteConnect API using extended logic to handle 100-day limit.\n",
        "\n",
        "        Args:\n",
        "            ticker (str): The trading symbol.\n",
        "            from_date (str): Start date in 'YYYY-MM-DD' format.\n",
        "            interval (str): Data interval (e.g., '15minute', 'day').\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A DataFrame containing OHLCV data.\n",
        "        \"\"\"\n",
        "        instrument = self.instrument_lookup(ticker)\n",
        "        if instrument == -1:\n",
        "            self.logger.warning(f\"Instrument token not found for ticker {ticker}.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        start_date = dt.datetime.strptime(from_date, '%Y-%m-%d')\n",
        "        end_date = dt.datetime.today()\n",
        "\n",
        "        self.logger.info(f\"Fetching {interval} data for {ticker} from {start_date.date()} to {end_date.date()}.\")\n",
        "\n",
        "        data = pd.DataFrame(columns=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "        while start_date < end_date:\n",
        "            fetch_end_date = min(start_date + dt.timedelta(days=100), end_date)\n",
        "            try:\n",
        "                data_chunk = pd.DataFrame(\n",
        "                    self.kite.historical_data(instrument, start_date, fetch_end_date, interval)\n",
        "                )\n",
        "                data = pd.concat([data, data_chunk], ignore_index=True)\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error fetching data for {ticker} from {start_date.date()} to {fetch_end_date.date()}: {e}\")\n",
        "                break\n",
        "            start_date = fetch_end_date\n",
        "\n",
        "        if not data.empty:\n",
        "            data.set_index(\"date\", inplace=True)\n",
        "            self.logger.info(f\"Fetched {len(data)} rows of data for {ticker}.\")\n",
        "            return data[['open', 'high', 'low', 'close', 'volume']]\n",
        "\n",
        "        self.logger.warning(f\"No data found for ticker {ticker}.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "    def fetch_data(self, scrip: str, from_date: str, to_date: str, interval: str = '15minute') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Wrapper for fetching historical data for a specific symbol.\n",
        "\n",
        "        Args:\n",
        "            scrip (str): The trading symbol (e.g., \"RELIANCE\").\n",
        "            from_date (str): Start date in 'YYYY-MM-DD' format.\n",
        "            to_date (str): End date in 'YYYY-MM-DD' format.\n",
        "            interval (str): Data interval (e.g., '15minute', 'day').\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A DataFrame containing OHLCV data.\n",
        "        \"\"\"\n",
        "        stripped_scrip = scrip.replace(\"NSE:\", \"\")\n",
        "        return self.fetch_ohlc_extended(stripped_scrip, from_date, interval)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_target_month(self) -> str:\n",
        "            \"\"\"\n",
        "            Determine the target month from the JSON (format yyyy-mm-01).\n",
        "            \"\"\"\n",
        "            today = datetime.now(IST)\n",
        "            target_month = (today + timedelta(days=1)).strftime('%Y-%m-01')  # yyyy-mm-01 for next month's 1st date\n",
        "            if target_month in self.symbols_cash_map:\n",
        "                self.logger.info(f\"Processing data for the target month: {target_month}\")\n",
        "                return target_month\n",
        "            else:\n",
        "                self.logger.error(f\"No data found for the target month: {target_month}. Exiting.\")\n",
        "                raise ValueError(f\"No data found for the target month: {target_month}\")\n",
        "\n",
        "\n",
        "    def get_symbols_for_target_month(self):\n",
        "        \"\"\"\n",
        "        Extract symbols for the target month.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of symbols for the target month. Returns an empty list if the target month is None.\n",
        "        \"\"\"\n",
        "        if self.target_month is None:\n",
        "            self.logger.warning(\"Target month is None. No symbols to extract.\")\n",
        "            return []  # Return an empty list if the target month is not set\n",
        "\n",
        "        # Extract symbols from the symbols_cash_map\n",
        "        return list(self.symbols_cash_map.get(self.target_month, {}).keys())\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    @njit\n",
        "    def get_final_bands_nb(close, upper, lower):\n",
        "        trend = np.full(close.shape, np.nan)\n",
        "        dir_ = np.full(close.shape, 1)\n",
        "        long = np.full(close.shape, np.nan)\n",
        "        short = np.full(close.shape, np.nan)\n",
        "\n",
        "        for i in range(1, close.shape[0]):\n",
        "            if close[i] > upper[i - 1]:\n",
        "                dir_[i] = 1\n",
        "            elif close[i] < lower[i - 1]:\n",
        "                dir_[i] = -1\n",
        "            else:\n",
        "                dir_[i] = dir_[i - 1]\n",
        "                if dir_[i] > 0 and lower[i] < lower[i - 1]:\n",
        "                    lower[i] = lower[i - 1]\n",
        "                if dir_[i] < 0 and upper[i] > upper[i - 1]:\n",
        "                    upper[i] = upper[i - 1]\n",
        "\n",
        "            if dir_[i] > 0:\n",
        "                trend[i] = long[i] = lower[i]\n",
        "            else:\n",
        "                trend[i] = short[i] = upper[i]\n",
        "\n",
        "        return trend, dir_, long, short\n",
        "\n",
        "    @staticmethod\n",
        "    def get_basic_bands(med_price, atr, multiplier):\n",
        "        matr = multiplier * atr\n",
        "        upper = med_price + matr\n",
        "        lower = med_price - matr\n",
        "        return upper, lower\n",
        "\n",
        "    def faster_supertrend_talib(self, high, low, close, period=7, multiplier=3):\n",
        "        avg_price = talib.MEDPRICE(high.flatten(), low.flatten())\n",
        "        atr = talib.ATR(high.flatten(), low.flatten(), close.flatten(), period)\n",
        "        upper, lower = self.get_basic_bands(avg_price, atr, multiplier)\n",
        "        return self.get_final_bands_nb(close, upper, lower)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def shift_array(arr: np.ndarray) -> np.ndarray:\n",
        "        shifted = np.zeros_like(arr)\n",
        "        shifted[1:] = arr[:-1]\n",
        "        return shifted\n",
        "\n",
        "\n",
        "\n",
        "    def MTF_Trend(self, high: pd.Series, low: pd.Series, close: pd.Series) -> np.ndarray:\n",
        "        # Resample to weekly and daily closes\n",
        "        close_1W = close.resample(\"1w\").last()\n",
        "        close_1D = close.resample(\"1d\").last().dropna().shift(1)\n",
        "\n",
        "        # Resample weekly and daily data to 15m\n",
        "        resampler_w = vbt.Resampler(close_1W.index, close.index, source_freq=\"W-SUN\", target_freq=\"15m\")\n",
        "        resampler_d = vbt.Resampler(close_1D.index, close.index, source_freq=\"1d\", target_freq=\"15m\")\n",
        "\n",
        "        # Compute multiple SuperTrend indicators\n",
        "        _, superd_1, _, _ = self.faster_supertrend_talib(high.values, low.values, close.values, 7, 3)\n",
        "        _, superd_2, _, _ = self.faster_supertrend_talib(high.values, low.values, close.values, 10, 3)\n",
        "        _, superd_3, _, _ = self.faster_supertrend_talib(high.values, low.values, close.values, 11, 2)\n",
        "\n",
        "        # Combine SuperTrend directions\n",
        "        concatenated = superd_1 + superd_2 + superd_3\n",
        "        shifted = self.shift_array(concatenated)\n",
        "\n",
        "        # Determine strong trend conditions\n",
        "        condition_3 = (concatenated == 3) & (shifted != 3)\n",
        "        condition_neg_3 = (concatenated == -3) & (shifted != -3)\n",
        "\n",
        "        # Define short-term trend\n",
        "        ST_Trend = np.where(condition_3, 1, 0)\n",
        "        ST_Trend = np.where(condition_neg_3, -1, ST_Trend)\n",
        "\n",
        "        # Weekly SMA and MACD-based conditions\n",
        "        SMA_1W = vbt.talib(\"SMA\").run(close_1W, timeperiod=20).real\n",
        "        MACD = vbt.talib_func(\"MACD\")\n",
        "        MACD_Hist = MACD(close_1W, 12, 26, 9)[2]\n",
        "        MACD_Positive = MACD_Hist > 0\n",
        "\n",
        "        Cls1W_Abv_SMA = close_1W.vbt >= SMA_1W\n",
        "\n",
        "        # Daily SMA alignment\n",
        "        SMA20_1D = vbt.talib(\"SMA\").run(close_1D, timeperiod=20).real\n",
        "        Close15mBelow20DMA = close.vbt <= vbt.talib(\"SMA\").run(close_1D, timeperiod=20).real.vbt.realign(resampler_d)\n",
        "\n",
        "        # Long-term trend (weekly SMA and MACD alignment)\n",
        "        LT_Trend = np.where(\n",
        "            np.logical_and(\n",
        "                MACD_Positive.vbt.realign(resampler_w).to_numpy(),\n",
        "                Cls1W_Abv_SMA.vbt.realign(resampler_w).to_numpy()\n",
        "            ),\n",
        "            1,\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # Combine short-term and long-term trends\n",
        "        MTF_Trend = np.where((LT_Trend == 1) & (ST_Trend == 1), 1, 0)\n",
        "        MTF_Trend = np.where(Close15mBelow20DMA.to_numpy(), -1, MTF_Trend)\n",
        "\n",
        "        return MTF_Trend\n",
        "\n",
        "\n",
        "    def fetch_data(self, scrip: str, from_date: str, to_date: str, interval: str = '15minute') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch historical candle data using KiteConnect API.\n",
        "\n",
        "        Args:\n",
        "            scrip (str): The trading symbol to fetch data for.\n",
        "            from_date (str): Start date in 'YYYY-MM-DD' format.\n",
        "            to_date (str): End date in 'YYYY-MM-DD' format.\n",
        "            interval (str): Timeframe for the candles. Default is '15minute'.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with OHLCV data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Strip \"NSE:\" prefix if present\n",
        "            stripped_scrip = scrip.replace(\"NSE:\", \"\")\n",
        "\n",
        "            # Retrieve the instrument token\n",
        "            instrument_token = self.instrument_lookup(stripped_scrip)\n",
        "            if instrument_token == -1:\n",
        "                self.logger.warning(f\"Instrument token not found for {scrip}.\")\n",
        "                return None\n",
        "\n",
        "            self.logger.info(f\"Fetching {interval} data for {stripped_scrip} from {from_date} to {to_date}.\")\n",
        "\n",
        "            start_date = datetime.strptime(from_date, '%Y-%m-%d')\n",
        "            end_date = datetime.strptime(to_date, '%Y-%m-%d')\n",
        "            data = pd.DataFrame(columns=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "            # Fetch data in chunks to handle API limitations\n",
        "            while start_date < end_date:\n",
        "                chunk_end_date = min(start_date + timedelta(days=100), end_date)\n",
        "                try:\n",
        "                    chunk = pd.DataFrame(\n",
        "                        self.kite.historical_data(\n",
        "                            instrument_token,\n",
        "                            from_date=start_date,\n",
        "                            to_date=chunk_end_date,\n",
        "                            interval=interval\n",
        "                        )\n",
        "                    )\n",
        "                    data = pd.concat([data, chunk], ignore_index=True)\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error fetching data for {scrip} between {start_date.date()} and {chunk_end_date.date()}: {e}\")\n",
        "                    break\n",
        "                start_date = chunk_end_date\n",
        "\n",
        "            if data.empty:\n",
        "                self.logger.warning(f\"No data available for {scrip}.\")\n",
        "                return None\n",
        "\n",
        "            # Post-process the data\n",
        "            data['date'] = pd.to_datetime(data['date'])\n",
        "            data.set_index('date', inplace=True)\n",
        "\n",
        "            self.logger.info(f\"Successfully fetched {len(data)} rows for {scrip}.\")\n",
        "            return data[['open', 'high', 'low', 'close', 'volume']]\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching data for {scrip}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def OrderGenerator(self, scrip: str, from_date: str, to_date: str):\n",
        "        \"\"\"\n",
        "        Generate orders for a specific scrip within the given date range.\n",
        "\n",
        "        Args:\n",
        "            scrip (str): The symbol for which to generate orders.\n",
        "            from_date (str): Start date in 'YYYY-MM-DD' format.\n",
        "            to_date (str): End date in 'YYYY-MM-DD' format.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame or None: A DataFrame of trades if successful, else None.\n",
        "        \"\"\"\n",
        "        # Retrieve `init_cash` value for the current scrip and target month\n",
        "        init_cash = self.symbols_cash_map.get(self.target_month, {}).get(scrip, self.default_cash)\n",
        "        self.logger.info(f\"Processing {scrip} for the target month {self.target_month} with init_cash={init_cash}.\")\n",
        "\n",
        "        # Fetch data for the specified range\n",
        "        # Ensure `to_date` does not exceed today's date\n",
        "        today = datetime.now(IST).strftime('%Y-%m-%d')\n",
        "        to_date = min(to_date, today)\n",
        "\n",
        "        data = self.fetch_data(scrip, from_date, to_date, interval='15minute')\n",
        "\n",
        "        if data is None or data.empty:\n",
        "            self.logger.warning(f\"No data available for {scrip} during {from_date} to {to_date}. Skipping.\")\n",
        "            return None\n",
        "\n",
        "        self.logger.info(f\"Data successfully fetched for {scrip} with {len(data)} rows.\")\n",
        "\n",
        "        # Ensure required columns exist\n",
        "        if not {'open', 'high', 'low', 'close'}.issubset(data.columns):\n",
        "            self.logger.error(f\"Data for {scrip} is missing required columns. Skipping.\")\n",
        "            return None\n",
        "\n",
        "        high, low, close = data['high'], data['low'], data['close']\n",
        "\n",
        "        # Generate signals using SuperTrend\n",
        "        Signals = vbt.IF(\n",
        "            class_name='SuperTrend',\n",
        "            short_name='st',\n",
        "            input_names=['high', 'low', 'close'],\n",
        "            param_names=[],\n",
        "            output_names=['value']\n",
        "        ).with_apply_func(\n",
        "            self.MTF_Trend, keep_pd=True\n",
        "        ).run(high, low, close)\n",
        "\n",
        "        # Create portfolio based on generated signals\n",
        "        pf = vbt.Portfolio.from_signals(\n",
        "            data,\n",
        "            long_entries=(Signals.value == 1),\n",
        "            long_exits=(Signals.value == -1),\n",
        "            stop_exit_price=\"close\",\n",
        "            sl_stop=data['low'].vbt.ago(125),\n",
        "            stop_ladder=\"uniform\",\n",
        "            tp_stop=vbt.Param([self.tp_ladder], keys=[\"tp_ladder_1\"]),\n",
        "            tsl_stop=0.2,\n",
        "            fees=0.0015,  # 0.15% of turnover\n",
        "            slippage=0.0005,  # 0.05% slippage\n",
        "            size_granularity=1,\n",
        "            max_size=init_cash,\n",
        "            init_cash=init_cash,\n",
        "            freq='15m'\n",
        "        )\n",
        "\n",
        "        # Extract readable trades data\n",
        "        df = pf.trades.readable\n",
        "        df[\"Column\"] = scrip\n",
        "\n",
        "        # Validate the DataFrame's structure\n",
        "        if df.empty or 'Entry Index' not in df.columns:\n",
        "            self.logger.warning(f\"No trades generated or 'Entry Index' missing for {scrip}.\")\n",
        "            return None\n",
        "\n",
        "        self.logger.info(f\"Processing completed for {scrip} in target month {self.target_month}.\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    def calculate_days_difference(self):\n",
        "        \"\"\"\n",
        "        Calculate the number of days from the target month's start to today, multiplied by a factor.\n",
        "        \"\"\"\n",
        "        # Ensure the current date is timezone-aware\n",
        "        today_date = datetime.now(IST)\n",
        "\n",
        "        # Convert target_month to a timezone-aware datetime\n",
        "        target_date = datetime.strptime(self.target_month, \"%Y-%m-%d\").replace(tzinfo=IST)\n",
        "\n",
        "        # Perform subtraction and calculate the difference in days\n",
        "        difference_days = (today_date - target_date).days\n",
        "\n",
        "        return difference_days * self.mult_factor\n",
        "\n",
        "\n",
        "    def exponential_backoff(self, function, *args, **kwargs):\n",
        "        retries = 0\n",
        "        delay = self.retry_delay\n",
        "        while retries < self.max_retries:\n",
        "            try:\n",
        "                return function(*args, **kwargs)\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "                self.logger.error(f\"Attempt {retries} failed with error: {e}\")\n",
        "                if retries >= self.max_retries:\n",
        "                    raise e\n",
        "                time.sleep(delay)\n",
        "                delay *= 2\n",
        "\n",
        "\n",
        "    def get_target_month(self, target_month=None):\n",
        "        \"\"\"\n",
        "        Validate the target month or calculate the default target month if not provided.\n",
        "        \"\"\"\n",
        "        if target_month is None:\n",
        "            # Default to the current month\n",
        "            target_month = datetime.now(IST).strftime('%Y-%m-01')  # First day of the current month\n",
        "\n",
        "        if target_month not in self.symbols_cash_map or not self.symbols_cash_map[target_month]:\n",
        "            self.logger.warning(f\"No data found for the target month: {target_month}. Skipping.\")\n",
        "            return None  # Indicate missing data\n",
        "        return target_month\n",
        "\n",
        "\n",
        "\n",
        "    def get_orderbook(self, max_months=12):\n",
        "        \"\"\"\n",
        "        Generate the order book by processing the latest `max_months` months in the JSON.\n",
        "        Runs OrderGenerator separately for each symbol for each target month.\n",
        "        \"\"\"\n",
        "        final_results = []  # List to store results for all months\n",
        "\n",
        "        # Get sorted months from the JSON keys (latest first)\n",
        "        sorted_months = sorted(self.symbols_cash_map.keys(), key=lambda x: datetime.strptime(x, \"%Y-%m-%d\"), reverse=True)\n",
        "        sorted_months = sorted_months[:max_months]  # Process only the first `max_months` months\n",
        "\n",
        "        # Process each target month\n",
        "        for month in sorted_months:\n",
        "            self.logger.info(f\"Processing data for the target month: {month}\")\n",
        "\n",
        "            # Validate the target month\n",
        "            target_month = self.get_target_month(month)\n",
        "            if not target_month:\n",
        "                continue  # Skip to the next month if no data is found\n",
        "\n",
        "            # Set up date ranges\n",
        "            month_start = IST.localize(datetime.strptime(target_month, \"%Y-%m-%d\"))\n",
        "            from_date = (month_start - timedelta(days=365)).strftime('%Y-%m-%d')  # Start 365 days before the target month\n",
        "            to_date = (month_start + timedelta(days=90)).strftime('%Y-%m-%d')  # End 90 days after the target month\n",
        "\n",
        "            # Ensure the `to_date` does not exceed today's date\n",
        "            today = datetime.now(IST).strftime('%Y-%m-%d')\n",
        "            to_date = min(to_date, today)\n",
        "\n",
        "            # Fetch and process symbols for the current month\n",
        "            symbols = self.symbols_cash_map.get(target_month, {}).keys()\n",
        "            if not symbols:\n",
        "                self.logger.warning(f\"No symbols found for the target month: {target_month}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            dfs = []\n",
        "            for symbol in symbols:\n",
        "                self.logger.info(f\"Running OrderGenerator for symbol: {symbol} in month: {target_month}.\")\n",
        "\n",
        "                try:\n",
        "                    # Fetch data and generate orders\n",
        "                    df = self.OrderGenerator(symbol, from_date=from_date, to_date=to_date)\n",
        "                    if df is not None and not df.empty:\n",
        "                        # Filter rows for the target month\n",
        "                        filtered_df = df[\n",
        "                            (df['Entry Index'] >= month_start) &\n",
        "                            (df['Entry Index'] < month_start + timedelta(days=31))\n",
        "                        ]\n",
        "\n",
        "                        if not filtered_df.empty:\n",
        "                            dfs.append(filtered_df)\n",
        "                        else:\n",
        "                            self.logger.info(f\"No relevant data found for symbol: {symbol} in the target month.\")\n",
        "                    else:\n",
        "                        self.logger.info(f\"No data returned by OrderGenerator for symbol: {symbol}.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing symbol {symbol} for month {target_month}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if dfs:\n",
        "                # Combine all symbols' dataframes for the current month\n",
        "                final_results.append(pd.concat(dfs, ignore_index=True))\n",
        "            else:\n",
        "                self.logger.warning(f\"No valid orders found for target month: {target_month}. Skipping.\")\n",
        "\n",
        "        if not final_results:\n",
        "            self.logger.info(\"No valid orders found for the specified months. Returning an empty orderbook.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Combine results for all processed months into a single DataFrame\n",
        "        final_orderbook = pd.concat(final_results, ignore_index=True)\n",
        "\n",
        "        # Log summary instead of full data\n",
        "        self.logger.info(f\"Order book generated successfully! Total trades: {len(final_orderbook)}\")\n",
        "        return final_orderbook\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "STEP3:::TRADE SUMMARY GENERATOR CLASS\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class TradeSummaryGenerator:\n",
        "    def __init__(self, logger):\n",
        "        \"\"\"\n",
        "        Initialize the TradeSummaryGenerator with Zerodha API credentials.\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "        # Default Zerodha API credentials\n",
        "        self.api_key = \"lwhfs47ilbvwc84g\"\n",
        "        self.access_token = \"ozWrM7XdI0CUbzzFiZQwaUfpSmqkblXY\"\n",
        "\n",
        "        # Initialize KiteConnect\n",
        "        self.kite = KiteConnect(api_key=self.api_key)\n",
        "        self.kite.set_access_token(self.access_token)\n",
        "\n",
        "    def fetch_equal_weighted_returns_monthly(self, symbols, month):\n",
        "        \"\"\"\n",
        "        Fetch monthly candle data for the given symbols and calculate equal-weighted returns.\n",
        "        Fallback to vbt.TVData if data is not available on Zerodha.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Fetching equal-weighted returns for month: {month}\")\n",
        "            monthly_returns = []\n",
        "\n",
        "            if not symbols:\n",
        "                self.logger.warning(f\"No symbols provided for month: {month}\")\n",
        "                return np.nan\n",
        "\n",
        "            # Convert month to datetime objects\n",
        "            current_month_start = pd.Timestamp(f\"{month}-01\", tz=\"Asia/Kolkata\")\n",
        "            previous_month_start = current_month_start - pd.offsets.MonthBegin(1)\n",
        "            current_month_end = current_month_start + pd.offsets.MonthEnd(0)\n",
        "\n",
        "            for symbol in symbols:\n",
        "                try:\n",
        "                    self.logger.info(f\"Pulling data for symbol: {symbol}\")\n",
        "                    instrument_token = self.get_instrument_token(symbol.replace(\"NSE:\", \"\"))\n",
        "\n",
        "                    if instrument_token:\n",
        "                        # Fetch data using Zerodha API\n",
        "                        data = self.kite.historical_data(\n",
        "                            instrument_token=instrument_token,\n",
        "                            from_date=previous_month_start.to_pydatetime(),\n",
        "                            to_date=current_month_end.to_pydatetime(),\n",
        "                            interval=\"day\"\n",
        "                        )\n",
        "\n",
        "                        # Convert data to DataFrame\n",
        "                        df = pd.DataFrame(data)\n",
        "                        if df.empty:\n",
        "                            self.logger.warning(f\"No data available for symbol: {symbol}\")\n",
        "                            continue\n",
        "\n",
        "                        df['date'] = pd.to_datetime(df['date'])\n",
        "                        if df['date'].dt.tz is None:\n",
        "                            df['date'] = df['date'].dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
        "                        else:\n",
        "                            df['date'] = df['date'].dt.tz_convert('Asia/Kolkata')\n",
        "\n",
        "                        df.set_index('date', inplace=True)\n",
        "\n",
        "                        # Get Close prices for the current and previous months\n",
        "                        previous_month_close = df.loc[df.index.to_period('M') == previous_month_start.to_period('M'), 'close']\n",
        "                        current_month_close = df.loc[df.index.to_period('M') == current_month_start.to_period('M'), 'close']\n",
        "\n",
        "                    else:\n",
        "                        # Fallback to vbt.TVData if no instrument token is found\n",
        "                        self.logger.warning(f\"Instrument token not found for symbol: {symbol}. Using vbt.TVData.\")\n",
        "                        data = vbt.TVData.pull(symbol, timeframe=\"1M\", tz=\"GMT+5:30\")\n",
        "                        close = data.get(\"Close\")\n",
        "\n",
        "                        if close.index.tz is None:\n",
        "                            close = close.tz_localize(\"UTC\").tz_convert(\"Asia/Kolkata\")\n",
        "                        else:\n",
        "                            close = close.tz_convert(\"Asia/Kolkata\")\n",
        "\n",
        "                        previous_month_close = close.loc[close.index.to_period('M') == previous_month_start.to_period('M')]\n",
        "                        current_month_close = close.loc[close.index.to_period('M') == current_month_start.to_period('M')]\n",
        "\n",
        "                    if not previous_month_close.empty and not current_month_close.empty:\n",
        "                        return_value = current_month_close.iloc[-1] / previous_month_close.iloc[-1] - 1\n",
        "                        monthly_returns.append(return_value)\n",
        "                        self.logger.info(f\"Return for symbol {symbol}: {return_value:.4f}\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Skipping symbol due to insufficient data: {symbol}, month: {month}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error fetching data for symbol: {symbol}, error: {e}\")\n",
        "\n",
        "            # Calculate equal-weighted return\n",
        "            if monthly_returns:\n",
        "                result = round(np.mean(monthly_returns), 4)\n",
        "                self.logger.info(f\"Equal-weighted return for month {month}: {result}\")\n",
        "                return result\n",
        "            else:\n",
        "                self.logger.warning(f\"No valid returns calculated for month: {month}\")\n",
        "                return np.nan\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching equal-weighted returns for month: {month}, error: {e}\")\n",
        "            return np.nan\n",
        "\n",
        "    def fetch_and_save_instruments(self, file_path=\"instruments.csv\"):\n",
        "        \"\"\"\n",
        "        Fetch all instruments from Zerodha and save them locally as a CSV file.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Fetching all instruments from Zerodha...\")\n",
        "            instruments = self.kite.instruments(\"NSE\")\n",
        "            instruments_df = pd.DataFrame(instruments)\n",
        "            instruments_df.to_csv(file_path, index=False)\n",
        "            self.logger.info(f\"Fetched and saved {len(instruments_df)} instruments to {file_path}.\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching instruments: {e}\")\n",
        "\n",
        "    def load_instruments(self, file_path=\"instruments.csv\"):\n",
        "        \"\"\"\n",
        "        Load instruments from a local CSV file into a DataFrame.\n",
        "        Fetch from Zerodha and save locally if the file does not exist.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Loading instruments from {file_path}...\")\n",
        "            instruments_df = pd.read_csv(file_path)\n",
        "            self.instrument_lookup = pd.Series(\n",
        "                instruments_df[\"instrument_token\"].values, index=instruments_df[\"tradingsymbol\"]\n",
        "            ).to_dict()\n",
        "            self.logger.info(f\"Loaded {len(self.instrument_lookup)} instruments from {file_path}.\")\n",
        "        except FileNotFoundError:\n",
        "            self.logger.warning(f\"{file_path} not found. Fetching from Zerodha...\")\n",
        "            self.fetch_and_save_instruments(file_path)\n",
        "            self.load_instruments(file_path)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading instruments from {file_path}: {e}\")\n",
        "            self.instrument_lookup = {}\n",
        "\n",
        "\n",
        "\n",
        "    def get_instrument_token(self, symbol):\n",
        "        \"\"\"\n",
        "        Retrieve the instrument token for a given symbol from the cached instruments.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Remove the 'NSE:' prefix if it exists\n",
        "            clean_symbol = symbol.replace(\"NSE:\", \"\")\n",
        "\n",
        "            if not hasattr(self, 'instrument_lookup') or not self.instrument_lookup:\n",
        "                self.load_instruments()\n",
        "\n",
        "            instrument_token = self.instrument_lookup.get(clean_symbol)\n",
        "            if instrument_token:\n",
        "                return instrument_token\n",
        "            else:\n",
        "                self.logger.warning(f\"Instrument token not found for symbol: {symbol}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching instrument token for symbol: {symbol}, error: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def fetch_cnxsmallcap_returns(self):\n",
        "        \"\"\"\n",
        "        Fetch monthly adjusted close prices for CNXSMALLCAP using Zerodha API and calculate monthly returns.\n",
        "        Fallback to vbt.TVData if data is not available on Zerodha.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Fetching CNXSMALLCAP returns...\")\n",
        "            instrument_token = self.get_instrument_token(\"CNXSMALLCAP\")\n",
        "            if not instrument_token:\n",
        "                self.logger.warning(\"Instrument token not found for CNXSMALLCAP. Using vbt.TVData fallback.\")\n",
        "                # Fallback to vbt.TVData\n",
        "                data = vbt.TVData.pull(\"NSE:CNXSMALLCAP\", timeframe=\"1M\", tz=\"GMT+5:30\")\n",
        "                close = data.get(\"Close\")\n",
        "\n",
        "                # Ensure timezone-awareness\n",
        "                if close.index.tz is None:\n",
        "                    close = close.tz_localize(\"UTC\").tz_convert(\"Asia/Kolkata\")\n",
        "                else:\n",
        "                    close = close.tz_convert(\"Asia/Kolkata\")\n",
        "\n",
        "                # Calculate monthly returns\n",
        "                cnxsmallcap_returns = close.pct_change().rename(\"CNXSMALLCAP_Returns\").to_frame()\n",
        "                cnxsmallcap_returns['Month'] = cnxsmallcap_returns.index.to_period('M')\n",
        "                self.logger.info(\"CNXSMALLCAP returns fetched using vbt.TVData.\")\n",
        "                return cnxsmallcap_returns\n",
        "\n",
        "            # Fetch data using Zerodha API\n",
        "            data = self.kite.historical_data(\n",
        "                instrument_token=instrument_token,\n",
        "                from_date=(datetime.now() - pd.DateOffset(years=5)).to_pydatetime(),\n",
        "                to_date=datetime.now(),\n",
        "                interval=\"month\"\n",
        "            )\n",
        "\n",
        "            # Convert data to DataFrame\n",
        "            df = pd.DataFrame(data)\n",
        "            if df.empty:\n",
        "                self.logger.warning(\"No data available for CNXSMALLCAP.\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df['date'] = pd.to_datetime(df['date']).dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
        "            df.set_index('date', inplace=True)\n",
        "\n",
        "            # Calculate monthly returns\n",
        "            cnxsmallcap_returns = df['close'].pct_change().rename(\"CNXSMALLCAP_Returns\").to_frame()\n",
        "            cnxsmallcap_returns['Month'] = cnxsmallcap_returns.index.to_period('M')\n",
        "            self.logger.info(\"CNXSMALLCAP returns fetched successfully.\")\n",
        "            return cnxsmallcap_returns\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching CNXSMALLCAP data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate_summary(self, tradebook_df, symbols_data, cnxsmallcap_returns):\n",
        "        \"\"\"\n",
        "        Generate a summary of trade metrics and merge with CNXSMALLCAP returns and cumulative equal-weighted returns.\n",
        "        Include an \"Overall\" row summarizing all months.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Generating summary from the tradebook...\")\n",
        "\n",
        "            # Ensure 'Month' column exists\n",
        "            if 'Month' not in tradebook_df.columns:\n",
        "                if 'Entry Index' in tradebook_df.columns:\n",
        "                    self.logger.info(\"Creating 'Month' column from 'Entry Index'.\")\n",
        "                    tradebook_df['Entry Index'] = pd.to_datetime(tradebook_df['Entry Index'], errors='coerce')\n",
        "                    tradebook_df['Month'] = tradebook_df['Entry Index'].dt.to_period('M')  # Extract month as Period[M]\n",
        "                else:\n",
        "                    self.logger.error(\"Missing required 'Entry Index' column to create 'Month'.\")\n",
        "                    raise KeyError(\"The 'Month' column is missing, and 'Entry Index' is not available to create it.\")\n",
        "\n",
        "            # Group by 'Month' and calculate metrics\n",
        "            summary = tradebook_df.groupby('Month').apply(self.calculate_metrics).reset_index()\n",
        "\n",
        "            # Add number of scrips from the symbols.json\n",
        "            summary['Num_Scrips_Input'] = summary['Month'].apply(\n",
        "                lambda x: len(symbols_data.get(str(x) + \"-01\", {}))\n",
        "            )\n",
        "\n",
        "            # Add interest adjustment for Num_Scrips_Input < 20\n",
        "            summary['Interest_PnL_Adjustment'] = summary['Num_Scrips_Input'].apply(\n",
        "                lambda x: 150_000 * 0.01 * (20 - x) if x < 20 else 0\n",
        "            )\n",
        "\n",
        "            # Add interest adjustment to Month_PnL\n",
        "            summary['Adjusted_Month_PnL'] = summary['Month_PnL'] + summary['Interest_PnL_Adjustment']\n",
        "\n",
        "            # Rename Adjusted_Month_PnL to Final_Month_PnL\n",
        "            summary.rename(columns={'Adjusted_Month_PnL': 'Final_Month_PnL'}, inplace=True)\n",
        "\n",
        "            # Calculate Strategy_Return with the new denominator\n",
        "            summary['Strategy_Return'] = round(summary['Final_Month_PnL'] / 3_000_000, 4)\n",
        "\n",
        "            # Calculate equal-weighted returns (without cost adjustment)\n",
        "            summary['Equal_Weighted_Return'] = summary['Month'].apply(\n",
        "                lambda month: self.fetch_equal_weighted_returns_monthly(\n",
        "                    symbols_data.get(str(month) + \"-01\", {}).keys(), str(month)\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Calculate equal-weighted returns (cost-adjusted by 0.4%)\n",
        "            summary['Equal_Weighted_Return_Cost_Adjusted'] = summary['Equal_Weighted_Return'] - 0.004\n",
        "\n",
        "            # Calculate cumulative equal-weighted returns\n",
        "            summary['Cumulative_Equal_Weighted_Return'] = round(\n",
        "                (1 + summary['Equal_Weighted_Return']).cumprod() - 1, 4\n",
        "            )\n",
        "\n",
        "            # Calculate cumulative equal-weighted returns (cost-adjusted by 0.4%)\n",
        "            summary['Cumulative_Equal_Weighted_Return_Cost_Adjusted'] = round(\n",
        "                (1 + summary['Equal_Weighted_Return_Cost_Adjusted']).cumprod() - 1, 4\n",
        "            )\n",
        "\n",
        "            # Add cumulative returns with reinvestments\n",
        "            summary['Strategy_Cumulative_Return_With_Reinvestments'] = round(\n",
        "                (1 + summary['Strategy_Return']).cumprod() - 1, 4\n",
        "            )\n",
        "\n",
        "            # Merge the CNXSMALLCAP monthly returns into the summary\n",
        "            summary = summary.merge(cnxsmallcap_returns, on='Month', how='left')\n",
        "\n",
        "            # Calculate cumulative returns for both CNXSMALLCAP and the strategy\n",
        "            summary['Cumulative_Strategy_Return'] = round((1 + summary['Strategy_Return']).cumprod() - 1, 4)\n",
        "            summary['Cumulative_CNXSMALLCAP_Return'] = round((1 + summary['CNXSMALLCAP_Returns']).cumprod() - 1, 4)\n",
        "\n",
        "            # Calculate drawdowns\n",
        "            summary['Max_Cumulative_Strategy_Return'] = summary['Cumulative_Strategy_Return'].cummax()\n",
        "            summary['Strategy_Drawdown'] = round(\n",
        "                (summary['Cumulative_Strategy_Return'] - summary['Max_Cumulative_Strategy_Return'])\n",
        "                / summary['Max_Cumulative_Strategy_Return'], 4\n",
        "            )\n",
        "\n",
        "            summary['Max_Cumulative_CNXSMALLCAP_Return'] = summary['Cumulative_CNXSMALLCAP_Return'].cummax()\n",
        "            summary['CNXSMALLCAP_Drawdown'] = round(\n",
        "                (summary['Cumulative_CNXSMALLCAP_Return'] - summary['Max_Cumulative_CNXSMALLCAP_Return'])\n",
        "                / summary['Max_Cumulative_CNXSMALLCAP_Return'], 4\n",
        "            )\n",
        "\n",
        "            # Drop unnecessary columns\n",
        "            summary = summary.drop(columns=[\n",
        "                'Max_Cumulative_Strategy_Return',\n",
        "                'Max_Cumulative_CNXSMALLCAP_Return'\n",
        "            ], errors='ignore')\n",
        "\n",
        "            # Calculate overall metrics and add as \"Overall\" row\n",
        "            overall_metrics = self.calculate_metrics(tradebook_df)\n",
        "            overall_metrics['Final_Month_PnL'] = summary['Final_Month_PnL'].sum()  # Add Overall Final Month PnL\n",
        "            overall_row = pd.DataFrame(overall_metrics).T\n",
        "            overall_row['Month'] = \"Overall\"\n",
        "\n",
        "            # Add \"Overall\" row to the top of the DataFrame\n",
        "            summary = pd.concat([overall_row, summary], ignore_index=True)\n",
        "\n",
        "            # Reorder columns to make Month the first column and Final_Month_PnL the second column\n",
        "            cols = ['Month', 'Final_Month_PnL'] + [col for col in summary.columns if col not in ['Month', 'Final_Month_PnL']]\n",
        "            summary = summary[cols]\n",
        "\n",
        "            self.logger.info(\"Summary generated successfully.\")\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating summary: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "    def calculate_metrics(self, group):\n",
        "        \"\"\"\n",
        "        Calculate additional metrics for the tradebook summary.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Calculating metrics for a trade group...\")\n",
        "        winning_trades = group[group['PnL'] > 0]\n",
        "        losing_trades = group[group['PnL'] < 0]\n",
        "\n",
        "        metrics = {\n",
        "            'Month_PnL': round(group['PnL'].sum(), 2),\n",
        "            'Month_Trades': len(group),\n",
        "            'Unique_Scrips': group['Column'].nunique(),\n",
        "            'Max_Loss': round(group['PnL'].min(), 2),\n",
        "            'Max_Profit': round(group['PnL'].max(), 2),\n",
        "            'Median_Profit': round(winning_trades['PnL'].median(), 2) if not winning_trades.empty else 0,\n",
        "            'Median_Loss': round(losing_trades['PnL'].median(), 2) if not losing_trades.empty else 0,\n",
        "            'Win_Rate': round(len(winning_trades) / len(group), 2) if len(group) > 0 else 0,\n",
        "            'Profit_Factor': round(winning_trades['PnL'].sum() / abs(losing_trades['PnL'].sum()), 2) if not losing_trades.empty else float('inf'),\n",
        "            'R_Multiple': round((winning_trades['Return'].mean() / abs(losing_trades['Return'].mean())), 2) if not losing_trades.empty else float('inf'),\n",
        "        }\n",
        "        self.logger.info(f\"Metrics calculated: {metrics}\")\n",
        "        return pd.Series(metrics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eDq9TvmPqZSC",
        "outputId": "2bfa537d-b063-4016-86f4-b8ab2bdc9812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-01T13:41:13.520571+05:30 - INFO - KiteConnect initialized successfully.\n",
            "KiteConnect initialized successfully.\n",
            "INFO:OrderBookGeneratorLogger:KiteConnect initialized successfully.\n",
            "2025-01-01T13:41:13.531209+05:30 - INFO - Loaded symbols JSON from /content/symbols.json\n",
            "Loaded symbols JSON from /content/symbols.json\n",
            "INFO:OrderBookGeneratorLogger:Loaded symbols JSON from /content/symbols.json\n",
            "2025-01-01T13:41:13.536253+05:30 - WARNING - No data found for the target month: 2025-01-01. Skipping.\n",
            "No data found for the target month: 2025-01-01. Skipping.\n",
            "WARNING:OrderBookGeneratorLogger:No data found for the target month: 2025-01-01. Skipping.\n",
            "2025-01-01T13:41:13.543016+05:30 - WARNING - No valid target month found. Symbols list will be empty.\n",
            "No valid target month found. Symbols list will be empty.\n",
            "WARNING:OrderBookGeneratorLogger:No valid target month found. Symbols list will be empty.\n",
            "2025-01-01T13:41:13.551407+05:30 - WARNING - Target month is None. No symbols to extract.\n",
            "Target month is None. No symbols to extract.\n",
            "WARNING:OrderBookGeneratorLogger:Target month is None. No symbols to extract.\n",
            "2025-01-01T13:41:13.555225+05:30 - INFO - Fetching NSE instruments from KiteConnect...\n",
            "Fetching NSE instruments from KiteConnect...\n",
            "INFO:OrderBookGeneratorLogger:Fetching NSE instruments from KiteConnect...\n",
            "2025-01-01T13:41:14.013636+05:30 - INFO - Fetched and saved NSE instrument dump.\n",
            "Fetched and saved NSE instrument dump.\n",
            "INFO:OrderBookGeneratorLogger:Fetched and saved NSE instrument dump.\n",
            "2025-01-01T13:41:14.030096+05:30 - INFO - Processing data for the target month: 2024-12-01\n",
            "Processing data for the target month: 2024-12-01\n",
            "INFO:OrderBookGeneratorLogger:Processing data for the target month: 2024-12-01\n",
            "2025-01-01T13:41:14.039626+05:30 - INFO - Running OrderGenerator for symbol: NSE:DEEPAKFERT in month: 2024-12-01.\n",
            "Running OrderGenerator for symbol: NSE:DEEPAKFERT in month: 2024-12-01.\n",
            "INFO:OrderBookGeneratorLogger:Running OrderGenerator for symbol: NSE:DEEPAKFERT in month: 2024-12-01.\n",
            "2025-01-01T13:41:14.047850+05:30 - INFO - Processing NSE:DEEPAKFERT for the target month None with init_cash=50000.\n",
            "Processing NSE:DEEPAKFERT for the target month None with init_cash=50000.\n",
            "INFO:OrderBookGeneratorLogger:Processing NSE:DEEPAKFERT for the target month None with init_cash=50000.\n",
            "2025-01-01T13:41:14.055728+05:30 - INFO - Fetching 15minute data for DEEPAKFERT from 2023-12-02 to 2025-01-01.\n",
            "Fetching 15minute data for DEEPAKFERT from 2023-12-02 to 2025-01-01.\n",
            "INFO:OrderBookGeneratorLogger:Fetching 15minute data for DEEPAKFERT from 2023-12-02 to 2025-01-01.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_orderbook method exists!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-01T13:41:15.975807+05:30 - INFO - Successfully fetched 6643 rows for NSE:DEEPAKFERT.\n",
            "Successfully fetched 6643 rows for NSE:DEEPAKFERT.\n",
            "INFO:OrderBookGeneratorLogger:Successfully fetched 6643 rows for NSE:DEEPAKFERT.\n",
            "2025-01-01T13:41:15.989189+05:30 - INFO - Data successfully fetched for NSE:DEEPAKFERT with 6643 rows.\n",
            "Data successfully fetched for NSE:DEEPAKFERT with 6643 rows.\n",
            "INFO:OrderBookGeneratorLogger:Data successfully fetched for NSE:DEEPAKFERT with 6643 rows.\n",
            "2025-01-01T13:41:17.774673+05:30 - INFO - Processing completed for NSE:DEEPAKFERT in target month None.\n",
            "Processing completed for NSE:DEEPAKFERT in target month None.\n",
            "INFO:OrderBookGeneratorLogger:Processing completed for NSE:DEEPAKFERT in target month None.\n",
            "2025-01-01T13:41:17.785846+05:30 - INFO - No relevant data found for symbol: NSE:DEEPAKFERT in the target month.\n",
            "No relevant data found for symbol: NSE:DEEPAKFERT in the target month.\n",
            "INFO:OrderBookGeneratorLogger:No relevant data found for symbol: NSE:DEEPAKFERT in the target month.\n",
            "2025-01-01T13:41:17.796559+05:30 - INFO - Running OrderGenerator for symbol: NSE:KALYANKJIL in month: 2024-12-01.\n",
            "Running OrderGenerator for symbol: NSE:KALYANKJIL in month: 2024-12-01.\n",
            "INFO:OrderBookGeneratorLogger:Running OrderGenerator for symbol: NSE:KALYANKJIL in month: 2024-12-01.\n",
            "2025-01-01T13:41:17.800485+05:30 - INFO - Processing NSE:KALYANKJIL for the target month None with init_cash=50000.\n",
            "Processing NSE:KALYANKJIL for the target month None with init_cash=50000.\n",
            "INFO:OrderBookGeneratorLogger:Processing NSE:KALYANKJIL for the target month None with init_cash=50000.\n",
            "2025-01-01T13:41:17.807491+05:30 - INFO - Fetching 15minute data for KALYANKJIL from 2023-12-02 to 2025-01-01.\n",
            "Fetching 15minute data for KALYANKJIL from 2023-12-02 to 2025-01-01.\n",
            "INFO:OrderBookGeneratorLogger:Fetching 15minute data for KALYANKJIL from 2023-12-02 to 2025-01-01.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-313b301c4417>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Generate the order book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOBGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_orderbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_months\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_month\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Order book generated successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-7913293dd95e>\u001b[0m in \u001b[0;36mget_orderbook\u001b[0;34m(self, max_months)\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                     \u001b[0;31m# Fetch data and generate orders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m                         \u001b[0;31m# Filter rows for the target month\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-7913293dd95e>\u001b[0m in \u001b[0;36mOrderGenerator\u001b[0;34m(self, scrip, from_date, to_date)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mto_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'15minute'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-7913293dd95e>\u001b[0m in \u001b[0;36mfetch_data\u001b[0;34m(self, scrip, from_date, to_date, interval)\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                     chunk = pd.DataFrame(\n\u001b[0;32m--> 446\u001b[0;31m                         self.kite.historical_data(\n\u001b[0m\u001b[1;32m    447\u001b[0m                             \u001b[0minstrument_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                             \u001b[0mfrom_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kiteconnect/connect.py\u001b[0m in \u001b[0;36mhistorical_data\u001b[0;34m(self, instrument_token, from_date, to_date, interval, continuous, oi)\u001b[0m\n\u001b[1;32m    640\u001b[0m                          })\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_historical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_historical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kiteconnect/connect.py\u001b[0m in \u001b[0;36m_format_historical\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"candles\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             record = {\n\u001b[0;32m--> 648\u001b[0;31m                 \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdateutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m                 \u001b[0;34m\"open\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;34m\"high\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                                       second=0, microsecond=0)\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipped_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_timelex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Splits the timestr into tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mskipped_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(cls, s)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mget_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mnextchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mnextchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mnextchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\x00'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mnextchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "FINAL STEP ::: CALLING THE SPECIFIC INSTANCES\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "log_file_path = f\"logfile_{datetime.now(IST).strftime('%Y-%m-%d')}.log\"\n",
        "\n",
        "# Setup logger\n",
        "logger = logging.getLogger(\"OrderBookGeneratorLogger\")\n",
        "logger.setLevel(logging.INFO)\n",
        "console_handler = logging.StreamHandler()\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the class\n",
        "OBGenerator = OrderBookGenerator(\n",
        "    logger=logger,\n",
        "    log_file=log_file_path,\n",
        "    symbols_json_path='/content/symbols.json',\n",
        "    api_key = api_key,\n",
        "    access_token=access_token\n",
        ")\n",
        "\n",
        "# Test if get_orderbook is recognized\n",
        "if hasattr(OBGenerator, 'get_orderbook'):\n",
        "    print(\"get_orderbook method exists!\")\n",
        "else:\n",
        "    print(\"get_orderbook method is missing!\")\n",
        "\n",
        "# Generate the order book\n",
        "try:\n",
        "    df = OBGenerator.get_orderbook(max_months=max_month)\n",
        "    print(\"Order book generated successfully!\")\n",
        "    print(df.head())\n",
        "except AttributeError as e:\n",
        "    print(f\"AttributeError: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error: {e}\")\n",
        "\n",
        "\n",
        "# Ensure datetime columns are properly formatted\n",
        "df[\"Entry Index\"] = pd.to_datetime(df[\"Entry Index\"])\n",
        "df[\"Exit Index\"] = pd.to_datetime(df[\"Exit Index\"], errors='coerce')  # Allow for NaT for open trades\n",
        "\n",
        "# The final DataFrame `df` contains the complete order book for the latest 12 months\n",
        "df.to_csv('tradebook.csv')\n",
        "\n",
        "\n",
        "# Create an instance of the class\n",
        "summary_generator = TradeSummaryGenerator(logger)\n",
        "\n",
        "# Example inputs\n",
        "tradebook_df = pd.read_csv(\"tradebook.csv\")  # Load the tradebook data\n",
        "symbols_data = json.load(open(\"symbols.json\"))  # Load the symbols.json\n",
        "cnxsmallcap_returns = summary_generator.fetch_cnxsmallcap_returns()\n",
        "\n",
        "# Generate the summary\n",
        "summary = summary_generator.generate_summary(tradebook_df, symbols_data, cnxsmallcap_returns)\n",
        "\n",
        "# Save the summary to a CSV\n",
        "summary.to_csv(\"summary.csv\", index=False)\n",
        "logger.info(\"Summary saved to summary.csv.\")\n",
        "\n",
        "\n",
        "\n",
        "summary\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMATW55QtX1TM26AZqEcbLG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}