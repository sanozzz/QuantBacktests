{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanozzz/QuantBacktests/blob/main/Trendlyne_Monthly_Rebalance(Zerodha).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variable\n",
        "\n",
        "api_key = \"lwhfs47ilbvwc84g\"\n",
        "access_token = \"ozWrM7XdI0CUbzzFiZQwaUfpSmqkblXY\"\n",
        "max_month = 60"
      ],
      "metadata": {
        "id": "bM5U99mN6TfY"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "6SB8YlhqrW0J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from pytz import timezone\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Define IST timezone\n",
        "IST = timezone('Asia/Kolkata')\n",
        "\n",
        "class ISTFormatter(logging.Formatter):\n",
        "    \"\"\"\n",
        "    Custom formatter to force log timestamps to use IST timezone.\n",
        "    \"\"\"\n",
        "    def formatTime(self, record, datefmt=None):\n",
        "        # Convert the record's created time to IST\n",
        "        record_time = datetime.fromtimestamp(record.created).astimezone(IST)\n",
        "        if datefmt:\n",
        "            return record_time.strftime(datefmt)\n",
        "        return record_time.isoformat()\n",
        "\n",
        "def setup_logger(log_file: str, level=logging.INFO, file_mode='a') -> logging.Logger:\n",
        "    \"\"\"\n",
        "    Sets up a logger with timestamps in IST for both file and console outputs.\n",
        "    Ensures no duplicate log handlers are added.\n",
        "\n",
        "    Args:\n",
        "        log_file (str): Path to the log file.\n",
        "        level (int): Logging level (e.g., logging.INFO, logging.DEBUG).\n",
        "        file_mode (str): Mode to open the log file ('a' for append, 'w' for overwrite).\n",
        "\n",
        "    Returns:\n",
        "        logging.Logger: Configured logger instance.\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(\"OrderBookGeneratorLogger\")\n",
        "    logger.setLevel(level)\n",
        "\n",
        "    # Clear existing handlers\n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    # Define the custom IST formatter\n",
        "    formatter = ISTFormatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # File handler\n",
        "    file_handler = logging.FileHandler(log_file, mode=file_mode)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    # Console handler\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "# Set the global timezone for the process (if on POSIX system)\n",
        "os.environ['TZ'] = 'Asia/Kolkata'\n",
        "try:\n",
        "    time.tzset()  # Apply timezone change globally (works on POSIX systems)\n",
        "except AttributeError:\n",
        "    # time.tzset() is not available on non-POSIX systems (e.g., Windows)\n",
        "    pass\n",
        "\n",
        "# Define the log file path\n",
        "log_file_path = f\"logfile_{datetime.now(IST).strftime('%Y-%m-%d')}.log\"\n",
        "logger = setup_logger(log_file_path, level=logging.INFO, file_mode='a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH93iIbLrJQ2",
        "outputId": "0e73021e-471a-4e3c-f8bc-c95eb1ab2083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file saved to symbols.json\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from pandas.tseries.offsets import MonthBegin\n",
        "\n",
        "# Load the CSV file\n",
        "input_file = \"BacktestInput.csv\"  # Replace with your actual file path\n",
        "df = pd.read_csv(input_file, on_bad_lines='skip')\n",
        "\n",
        "# Step 1: Delete all rows with NaN values\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "# Step 2: Drop unnecessary columns\n",
        "columns_to_drop = [\n",
        "    \"Start Price\", \"End Price\", \"Change\",\n",
        "    \"Avg Change %\", \"Weightage of Each stock %\", \"NAV (Initial 100)\"\n",
        "]\n",
        "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Step 3: Forward fill the \"As on Date\" column for rows without a date\n",
        "df['As on Date'] = df['As on Date'].fillna(method='ffill')\n",
        "\n",
        "# Rename \"As on Date\" to \"Date\"\n",
        "df.rename(columns={\"As on Date\": \"Date\"}, inplace=True)\n",
        "\n",
        "# Step 4: Convert the date range (e.g., 2012-09-28 to 2012-10-31) to the first of the last date\n",
        "df['Date'] = df['Date'].str.extract(r'to\\s+(\\d{4}-\\d{2}-\\d{2})')  # Extract the last date\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce') - MonthBegin(1)  # Convert to first of the month\n",
        "\n",
        "# Step 5: Keep only rows where 'Stock' column is not blank\n",
        "df = df.dropna(subset=['Stock'])\n",
        "\n",
        "# Step 6: Extract the first part of the stock name and add 'NSE:' as prefix\n",
        "df['Stock'] = df['Stock'].str.extract(r'^([A-Z&-]+)').fillna('')  # Extract first part\n",
        "df['Stock'] = 'NSE:' + df['Stock']  # Add NSE prefix\n",
        "\n",
        "# Step 7: Convert the cleaned DataFrame to a JSON structure\n",
        "json_output = {}\n",
        "for _, row in df.iterrows():\n",
        "    date = row['Date'].strftime('%Y-%m-%d')  # Ensure date format\n",
        "    stock = row['Stock']\n",
        "    if date not in json_output:\n",
        "        json_output[date] = {}\n",
        "    json_output[date][stock] = 150000  # Assign default cash amount\n",
        "\n",
        "# Save the final JSON to symbols.json\n",
        "json_file = \"symbols.json\"\n",
        "with open(json_file, \"w\") as f:\n",
        "    json.dump(json_output, f, indent=4)\n",
        "\n",
        "print(f\"JSON file saved to {json_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "BZCDrJXw1Q4X"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/vectorbt.pro')  # Add the path where your package is cloned\n",
        "import vectorbtpro as vbt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import talib\n",
        "from numba import njit\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import logging\n",
        "from pytz import timezone as tz\n",
        "import requests\n",
        "from kiteconnect import KiteConnect\n",
        "\n",
        "# Define IST timezone\n",
        "IST = tz('Asia/Kolkata')\n",
        "\n",
        "class OrderBookGenerator:\n",
        "    def __init__(self, logger, api_key, access_token, log_file, symbols_json_path='/content/symbols.json', default_cash=50000,\n",
        "                 tp_ladder=[0.2,1], mult_factor=25, max_retries=5, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Initialize the OrderBookGenerator class.\n",
        "\n",
        "        Args:\n",
        "            logger: Logger object for logging.\n",
        "            api_key (str): Kite API key.\n",
        "            access_token (str): Kite access token.\n",
        "            log_file (str): Path to the log file.\n",
        "            symbols_json_path (str): Path to the JSON file containing symbol mappings.\n",
        "            default_cash (int): Default initial cash value for symbols.\n",
        "            tp_ladder (list): Take-profit ladder values.\n",
        "            mult_factor (int): Multiplier factor for calculations.\n",
        "            max_retries (int): Maximum retries for API calls.\n",
        "            retry_delay (int): Delay between retries (in seconds).\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "        self.log_file = log_file\n",
        "        self.tp_ladder = tp_ladder\n",
        "        self.mult_factor = mult_factor\n",
        "        self.max_retries = max_retries\n",
        "        self.retry_delay = retry_delay\n",
        "        self.default_cash = default_cash\n",
        "\n",
        "        # Initialize KiteConnect\n",
        "        try:\n",
        "            self.kite = KiteConnect(api_key=api_key)\n",
        "            self.kite.set_access_token(access_token)\n",
        "            self.logger.info(\"KiteConnect initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error initializing KiteConnect: {e}\")\n",
        "            raise e\n",
        "\n",
        "        # Load symbols and their respective cash values\n",
        "        self.symbols_cash_map = self.load_symbols_cash(symbols_json_path)\n",
        "        self.target_month = self.get_target_month()\n",
        "        self.symbols_list = self.get_symbols_for_target_month()\n",
        "\n",
        "        # Fetch instrument data from Kite API\n",
        "        self.instrument_df = self.fetch_instruments()\n",
        "\n",
        "        warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "    def fetch_instruments(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch the list of all NSE instruments from KiteConnect API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Fetching NSE instruments from KiteConnect...\")\n",
        "            instrument_dump = self.kite.instruments(\"NSE\")\n",
        "            instrument_df = pd.DataFrame(instrument_dump)\n",
        "            instrument_df.to_csv(\"NSE_Instruments.csv\", index=False)  # Save as CSV for reference\n",
        "            self.logger.info(\"Fetched and saved NSE instrument dump.\")\n",
        "            return instrument_df\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching instruments: {e}\")\n",
        "            raise e\n",
        "\n",
        "\n",
        "\n",
        "    def load_symbols_cash(self, symbols_json_path: str) -> dict:\n",
        "        \"\"\"\n",
        "        Load the JSON file mapping months to symbols with cash values.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(symbols_json_path, 'r') as f:\n",
        "                symbols_cash_map = json.load(f)\n",
        "                self.logger.info(f\"Loaded symbols JSON from {symbols_json_path}\")\n",
        "                return symbols_cash_map\n",
        "        except FileNotFoundError:\n",
        "            self.logger.error(f\"Symbols JSON file not found: {symbols_json_path}. Exiting.\")\n",
        "            raise FileNotFoundError(f\"Symbols JSON file not found: {symbols_json_path}\")\n",
        "\n",
        "\n",
        "\n",
        "    def instrument_lookup(self, symbol: str) -> int:\n",
        "        \"\"\"\n",
        "        Looks up the instrument token for a given symbol from the instrument dump.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            token = self.instrument_df[self.instrument_df['tradingsymbol'] == symbol].instrument_token.values[0]\n",
        "            self.logger.info(f\"Found instrument token for {symbol}: {token}\")\n",
        "            return token\n",
        "        except IndexError:\n",
        "            self.logger.warning(f\"Instrument token not found for symbol: {symbol}.\")\n",
        "            return -1\n",
        "\n",
        "    def fetch_ohlc_extended(self, ticker: str, from_date: str, interval: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch historical data from Zerodha's KiteConnect API using extended logic to handle 100-day limit.\n",
        "\n",
        "        Args:\n",
        "            ticker (str): The trading symbol.\n",
        "            from_date (str): Start date in 'YYYY-MM-DD' format.\n",
        "            interval (str): Data interval (e.g., '15minute', 'day').\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A DataFrame containing OHLCV data.\n",
        "        \"\"\"\n",
        "        instrument = self.instrument_lookup(ticker)\n",
        "        if instrument == -1:\n",
        "            self.logger.warning(f\"No instrument token found for ticker {ticker}. Skipping.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        start_date = dt.datetime.strptime(from_date, '%Y-%m-%d')\n",
        "        end_date = dt.datetime.today()\n",
        "\n",
        "        self.logger.info(f\"Fetching {interval} data for {ticker} using KiteConnect from {start_date.date()} to {end_date.date()}...\")\n",
        "\n",
        "        data = pd.DataFrame(columns=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "        while start_date < end_date:\n",
        "            fetch_end_date = min(start_date + dt.timedelta(days=100), end_date)\n",
        "            try:\n",
        "                data_chunk = pd.DataFrame(\n",
        "                    self.kite.historical_data(instrument, start_date, fetch_end_date, interval)\n",
        "                )\n",
        "                data = pd.concat([data, data_chunk], ignore_index=True)\n",
        "                self.logger.info(f\"Fetched data from {start_date.date()} to {fetch_end_date.date()}\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error fetching data for {ticker} from {start_date.date()} to {fetch_end_date.date()}: {e}\")\n",
        "                break\n",
        "            start_date = fetch_end_date\n",
        "\n",
        "        if not data.empty:\n",
        "            data.set_index(\"date\", inplace=True)\n",
        "            self.logger.info(f\"Fetched {len(data)} rows of data for {ticker}.\")\n",
        "            return data[['open', 'high', 'low', 'close', 'volume']]\n",
        "\n",
        "        self.logger.warning(f\"No data found for ticker {ticker}.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def fetch_data(self, scrip: str, from_date: str, to_date: str, interval: str = '15minute') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Wrapper for fetching historical data for a specific symbol.\n",
        "\n",
        "        Args:\n",
        "            scrip (str): The trading symbol (e.g., \"RELIANCE\").\n",
        "            from_date (str): Start date in 'YYYY-MM-DD' format.\n",
        "            to_date (str): End date in 'YYYY-MM-DD' format.\n",
        "            interval (str): Data interval (e.g., '15minute', 'day').\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A DataFrame containing OHLCV data.\n",
        "        \"\"\"\n",
        "        stripped_scrip = scrip.replace(\"NSE:\", \"\")\n",
        "        return self.fetch_ohlc_extended(stripped_scrip, from_date, interval)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_target_month(self) -> str:\n",
        "            \"\"\"\n",
        "            Determine the target month from the JSON (format yyyy-mm-01).\n",
        "            \"\"\"\n",
        "            today = datetime.now(IST)\n",
        "            target_month = (today + timedelta(days=1)).strftime('%Y-%m-01')  # yyyy-mm-01 for next month's 1st date\n",
        "            if target_month in self.symbols_cash_map:\n",
        "                self.logger.info(f\"Processing data for the target month: {target_month}\")\n",
        "                return target_month\n",
        "            else:\n",
        "                self.logger.error(f\"No data found for the target month: {target_month}. Exiting.\")\n",
        "                raise ValueError(f\"No data found for the target month: {target_month}\")\n",
        "\n",
        "\n",
        "    def get_symbols_for_target_month(self) -> list:\n",
        "        \"\"\"\n",
        "        Extract symbols for the target month.\n",
        "        \"\"\"\n",
        "        return list(self.symbols_cash_map[self.target_month].keys())\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    @njit\n",
        "    def get_final_bands_nb(close, upper, lower):\n",
        "        trend = np.full(close.shape, np.nan)\n",
        "        dir_ = np.full(close.shape, 1)\n",
        "        long = np.full(close.shape, np.nan)\n",
        "        short = np.full(close.shape, np.nan)\n",
        "\n",
        "        for i in range(1, close.shape[0]):\n",
        "            if close[i] > upper[i - 1]:\n",
        "                dir_[i] = 1\n",
        "            elif close[i] < lower[i - 1]:\n",
        "                dir_[i] = -1\n",
        "            else:\n",
        "                dir_[i] = dir_[i - 1]\n",
        "                if dir_[i] > 0 and lower[i] < lower[i - 1]:\n",
        "                    lower[i] = lower[i - 1]\n",
        "                if dir_[i] < 0 and upper[i] > upper[i - 1]:\n",
        "                    upper[i] = upper[i - 1]\n",
        "\n",
        "            if dir_[i] > 0:\n",
        "                trend[i] = long[i] = lower[i]\n",
        "            else:\n",
        "                trend[i] = short[i] = upper[i]\n",
        "\n",
        "        return trend, dir_, long, short\n",
        "\n",
        "    @staticmethod\n",
        "    def get_basic_bands(med_price, atr, multiplier):\n",
        "        matr = multiplier * atr\n",
        "        upper = med_price + matr\n",
        "        lower = med_price - matr\n",
        "        return upper, lower\n",
        "\n",
        "    def faster_supertrend_talib(self, high, low, close, period=7, multiplier=3):\n",
        "        avg_price = talib.MEDPRICE(high.flatten(), low.flatten())\n",
        "        atr = talib.ATR(high.flatten(), low.flatten(), close.flatten(), period)\n",
        "        upper, lower = self.get_basic_bands(avg_price, atr, multiplier)\n",
        "        return self.get_final_bands_nb(close, upper, lower)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def shift_array(arr: np.ndarray) -> np.ndarray:\n",
        "        shifted = np.zeros_like(arr)\n",
        "        shifted[1:] = arr[:-1]\n",
        "        return shifted\n",
        "\n",
        "\n",
        "\n",
        "    def MTF_Trend(self, high: pd.Series, low: pd.Series, close: pd.Series) -> np.ndarray:\n",
        "        # Resample to weekly and daily closes\n",
        "        close_1W = close.resample(\"1w\").last()\n",
        "        close_1D = close.resample(\"1d\").last().dropna().shift(1)\n",
        "\n",
        "        # Resample weekly and daily data to 15m\n",
        "        resampler_w = vbt.Resampler(close_1W.index, close.index, source_freq=\"W-SUN\", target_freq=\"15m\")\n",
        "        resampler_d = vbt.Resampler(close_1D.index, close.index, source_freq=\"1d\", target_freq=\"15m\")\n",
        "\n",
        "        # Compute multiple SuperTrend indicators\n",
        "        _, superd_1, _, _ = self.faster_supertrend_talib(high.values, low.values, close.values, 7, 3)\n",
        "        _, superd_2, _, _ = self.faster_supertrend_talib(high.values, low.values, close.values, 10, 3)\n",
        "        _, superd_3, _, _ = self.faster_supertrend_talib(high.values, low.values, close.values, 11, 2)\n",
        "\n",
        "        # Combine SuperTrend directions\n",
        "        concatenated = superd_1 + superd_2 + superd_3\n",
        "        shifted = self.shift_array(concatenated)\n",
        "\n",
        "        # Determine strong trend conditions\n",
        "        condition_3 = (concatenated == 3) & (shifted != 3)\n",
        "        condition_neg_3 = (concatenated == -3) & (shifted != -3)\n",
        "\n",
        "        # Define short-term trend\n",
        "        ST_Trend = np.where(condition_3, 1, 0)\n",
        "        ST_Trend = np.where(condition_neg_3, -1, ST_Trend)\n",
        "\n",
        "        # Weekly SMA and MACD-based conditions\n",
        "        SMA_1W = vbt.talib(\"SMA\").run(close_1W, timeperiod=20).real\n",
        "        MACD = vbt.talib_func(\"MACD\")\n",
        "        MACD_Hist = MACD(close_1W, 12, 26, 9)[2]\n",
        "        MACD_Positive = MACD_Hist > 0\n",
        "\n",
        "        Cls1W_Abv_SMA = close_1W.vbt >= SMA_1W\n",
        "\n",
        "        # Daily SMA alignment\n",
        "        SMA20_1D = vbt.talib(\"SMA\").run(close_1D, timeperiod=20).real\n",
        "        Close15mBelow20DMA = close.vbt <= vbt.talib(\"SMA\").run(close_1D, timeperiod=20).real.vbt.realign(resampler_d)\n",
        "\n",
        "        # Long-term trend (weekly SMA and MACD alignment)\n",
        "        LT_Trend = np.where(\n",
        "            np.logical_and(\n",
        "                MACD_Positive.vbt.realign(resampler_w).to_numpy(),\n",
        "                Cls1W_Abv_SMA.vbt.realign(resampler_w).to_numpy()\n",
        "            ),\n",
        "            1,\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # Combine short-term and long-term trends\n",
        "        MTF_Trend = np.where((LT_Trend == 1) & (ST_Trend == 1), 1, 0)\n",
        "        MTF_Trend = np.where(Close15mBelow20DMA.to_numpy(), -1, MTF_Trend)\n",
        "\n",
        "        return MTF_Trend\n",
        "\n",
        "\n",
        "    def fetch_data(self, scrip: str, from_date: str, to_date: str, interval: str = '15minute') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch historical candle data using KiteConnect API.\n",
        "\n",
        "        Args:\n",
        "            scrip (str): The trading symbol to fetch data for.\n",
        "            from_date (str): Start date in 'YYYY-MM-DD' format.\n",
        "            to_date (str): End date in 'YYYY-MM-DD' format.\n",
        "            interval (str): Timeframe for the candles. Default is '15minute'.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with OHLCV data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Strip \"NSE:\" prefix if present\n",
        "            stripped_scrip = scrip.replace(\"NSE:\", \"\")\n",
        "\n",
        "            # Retrieve the instrument token\n",
        "            instrument_token = self.instrument_lookup(stripped_scrip)\n",
        "            if instrument_token == -1:\n",
        "                self.logger.warning(f\"Instrument token not found for scrip: {scrip}. Skipping.\")\n",
        "                return None\n",
        "\n",
        "            self.logger.info(f\"Fetching {interval} data for {stripped_scrip} using KiteConnect from {from_date} to {to_date}...\")\n",
        "\n",
        "            start_date = datetime.strptime(from_date, '%Y-%m-%d')\n",
        "            end_date = datetime.strptime(to_date, '%Y-%m-%d')\n",
        "            data = pd.DataFrame(columns=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "            # Fetch data in chunks to handle API limitations\n",
        "            while start_date < end_date:\n",
        "                chunk_end_date = min(start_date + timedelta(days=100), end_date)\n",
        "                try:\n",
        "                    chunk = pd.DataFrame(\n",
        "                        self.kite.historical_data(\n",
        "                            instrument_token,\n",
        "                            from_date=start_date,\n",
        "                            to_date=chunk_end_date,\n",
        "                            interval='15minute' if interval == '15minute' else interval\n",
        "                        )\n",
        "                    )\n",
        "                    data = pd.concat([data, chunk], ignore_index=True)\n",
        "                    self.logger.info(f\"Fetched data from {start_date.date()} to {chunk_end_date.date()} for {stripped_scrip}.\")\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error fetching data for {scrip} from {start_date.date()} to {chunk_end_date.date()}: {e}\")\n",
        "                    break\n",
        "                start_date = chunk_end_date\n",
        "\n",
        "            if data.empty:\n",
        "                self.logger.warning(f\"No data found for scrip: {scrip}.\")\n",
        "                return None\n",
        "\n",
        "            # Post-process the data\n",
        "            data['date'] = pd.to_datetime(data['date'])\n",
        "            data.set_index('date', inplace=True)\n",
        "\n",
        "            self.logger.info(f\"Data fetched successfully for {scrip}. Total rows: {len(data)}.\")\n",
        "            return data[['open', 'high', 'low', 'close', 'volume']]\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching data for {scrip}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def OrderGenerator(self, scrip: str, from_date: str, to_date: str):\n",
        "        \"\"\"\n",
        "        Generate orders for a specific scrip within the given date range.\n",
        "\n",
        "        Args:\n",
        "            scrip (str): The symbol for which to generate orders.\n",
        "            from_date (str): Start date in 'YYYY-MM-DD' format.\n",
        "            to_date (str): End date in 'YYYY-MM-DD' format.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame or None: A DataFrame of trades if successful, else None.\n",
        "        \"\"\"\n",
        "        # Retrieve `init_cash` value for the current scrip and target month\n",
        "        init_cash = self.symbols_cash_map.get(self.target_month, {}).get(scrip, self.default_cash)\n",
        "        self.logger.info(f\"Processing {scrip} for the target month {self.target_month} with init_cash={init_cash}.\")\n",
        "\n",
        "        # Fetch data for the specified range\n",
        "        # Ensure `to_date` does not exceed today's date\n",
        "        today = datetime.now(IST).strftime('%Y-%m-%d')\n",
        "        to_date = min(to_date, today)\n",
        "\n",
        "        data = self.fetch_data(scrip, from_date, to_date, interval='15minute')\n",
        "\n",
        "        if data is None or data.empty:\n",
        "            self.logger.warning(f\"No data available for {scrip} during {from_date} to {to_date}. Skipping.\")\n",
        "            return None\n",
        "\n",
        "        self.logger.info(f\"Data successfully fetched for {scrip} with {len(data)} rows.\")\n",
        "\n",
        "        # Ensure required columns exist\n",
        "        if not {'open', 'high', 'low', 'close'}.issubset(data.columns):\n",
        "            self.logger.error(f\"Data for {scrip} is missing required columns. Skipping.\")\n",
        "            return None\n",
        "\n",
        "        high, low, close = data['high'], data['low'], data['close']\n",
        "\n",
        "        # Generate signals using SuperTrend\n",
        "        Signals = vbt.IF(\n",
        "            class_name='SuperTrend',\n",
        "            short_name='st',\n",
        "            input_names=['high', 'low', 'close'],\n",
        "            param_names=[],\n",
        "            output_names=['value']\n",
        "        ).with_apply_func(\n",
        "            self.MTF_Trend, keep_pd=True\n",
        "        ).run(high, low, close)\n",
        "\n",
        "        # Create portfolio based on generated signals\n",
        "        pf = vbt.Portfolio.from_signals(\n",
        "            data,\n",
        "            long_entries=(Signals.value == 1),\n",
        "            long_exits=(Signals.value == -1),\n",
        "            stop_exit_price=\"close\",\n",
        "            sl_stop=data['low'].vbt.ago(125),\n",
        "            stop_ladder=\"uniform\",\n",
        "            tp_stop=vbt.Param([self.tp_ladder], keys=[\"tp_ladder_1\"]),\n",
        "            tsl_stop=0.2,\n",
        "            fees=0.0015,  # 0.15% of turnover\n",
        "            slippage=0.0005,  # 0.05% slippage\n",
        "            size_granularity=1,\n",
        "            max_size=init_cash,\n",
        "            init_cash=init_cash,\n",
        "            freq='15m'\n",
        "        )\n",
        "\n",
        "        # Extract readable trades data\n",
        "        df = pf.trades.readable\n",
        "        df[\"Column\"] = scrip\n",
        "\n",
        "        # Validate the DataFrame's structure\n",
        "        if df.empty or 'Entry Index' not in df.columns:\n",
        "            self.logger.warning(f\"No trades generated or 'Entry Index' missing for {scrip}.\")\n",
        "            return None\n",
        "\n",
        "        self.logger.info(f\"Processing completed for {scrip} in target month {self.target_month}.\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    def calculate_days_difference(self):\n",
        "        \"\"\"\n",
        "        Calculate the number of days from the target month's start to today, multiplied by a factor.\n",
        "        \"\"\"\n",
        "        # Ensure the current date is timezone-aware\n",
        "        today_date = datetime.now(IST)\n",
        "\n",
        "        # Convert target_month to a timezone-aware datetime\n",
        "        target_date = datetime.strptime(self.target_month, \"%Y-%m-%d\").replace(tzinfo=IST)\n",
        "\n",
        "        # Perform subtraction and calculate the difference in days\n",
        "        difference_days = (today_date - target_date).days\n",
        "\n",
        "        return difference_days * self.mult_factor\n",
        "\n",
        "\n",
        "    def exponential_backoff(self, function, *args, **kwargs):\n",
        "        retries = 0\n",
        "        delay = self.retry_delay\n",
        "        while retries < self.max_retries:\n",
        "            try:\n",
        "                return function(*args, **kwargs)\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "                self.logger.error(f\"Attempt {retries} failed with error: {e}\")\n",
        "                if retries >= self.max_retries:\n",
        "                    raise e\n",
        "                time.sleep(delay)\n",
        "                delay *= 2\n",
        "\n",
        "\n",
        "\n",
        "    def get_orderbook(self, max_months=12):\n",
        "        \"\"\"\n",
        "        Generate the order book by processing the latest `max_months` months in the JSON.\n",
        "        Runs OrderGenerator separately for each symbol for each target month.\n",
        "        \"\"\"\n",
        "        final_results = []  # List to store results for all months\n",
        "\n",
        "        # Get sorted months from the JSON keys (latest first)\n",
        "        sorted_months = sorted(self.symbols_cash_map.keys(), key=lambda x: datetime.strptime(x, \"%Y-%m-%d\"), reverse=True)\n",
        "        sorted_months = sorted_months[:max_months]  # Process only the first `max_months` months\n",
        "\n",
        "        # Process each target month\n",
        "        for target_month in sorted_months:\n",
        "            self.logger.info(f\"Processing data for the target month: {target_month}\")\n",
        "            self.target_month = target_month  # Set the target month for the current loop\n",
        "\n",
        "            month_start = IST.localize(datetime.strptime(target_month, \"%Y-%m-%d\"))\n",
        "            from_date = (month_start - timedelta(days=365)).strftime('%Y-%m-%d')  # Start 365 days before the target month\n",
        "            to_date = (month_start + timedelta(days=90)).strftime('%Y-%m-%d')  # End 90 days after the target month\n",
        "\n",
        "            # Ensure the `to_date` does not exceed today's date\n",
        "            today = datetime.now(IST).strftime('%Y-%m-%d')\n",
        "            to_date = min(to_date, today)\n",
        "\n",
        "            # Process each symbol for the target month\n",
        "            if target_month not in self.symbols_cash_map:\n",
        "                self.logger.warning(f\"No data found for the target month: {target_month}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            symbols = self.symbols_cash_map[target_month].keys()\n",
        "            dfs = []\n",
        "            for symbol in symbols:\n",
        "                self.logger.info(f\"Running OrderGenerator for symbol: {symbol} in month: {target_month}.\")\n",
        "\n",
        "                # Fetch data and generate orders\n",
        "                df = self.OrderGenerator(symbol, from_date=from_date, to_date=to_date)\n",
        "                if df is not None and not df.empty:\n",
        "                    # Filter rows for the target month\n",
        "                    filtered_df = df[\n",
        "                        (df['Entry Index'] >= month_start) &\n",
        "                        (df['Entry Index'] < month_start + timedelta(days=31))\n",
        "                    ]\n",
        "\n",
        "                    if not filtered_df.empty:\n",
        "                        dfs.append(filtered_df)\n",
        "\n",
        "            if dfs:\n",
        "                # Combine all symbols' dataframes for the current month\n",
        "                final_results.append(pd.concat(dfs, ignore_index=True))\n",
        "            else:\n",
        "                self.logger.warning(f\"No valid orders found for target month: {target_month}. Skipping.\")\n",
        "\n",
        "        if not final_results:\n",
        "            self.logger.info(\"No valid orders found for the specified months. Returning an empty orderbook.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Combine results for all processed months into a single DataFrame\n",
        "        final_orderbook = pd.concat(final_results, ignore_index=True)\n",
        "\n",
        "        # Log summary instead of full data\n",
        "        self.logger.info(f\"Order book generated successfully! Total trades: {len(final_orderbook)}\")\n",
        "        return final_orderbook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kiteconnect import KiteConnect\n",
        "import vectorbtpro as vbt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "\n",
        "\n",
        "class TradeSummaryGenerator:\n",
        "    def __init__(self, logger):\n",
        "        \"\"\"\n",
        "        Initialize the TradeSummaryGenerator with Zerodha API credentials.\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "        # Default Zerodha API credentials\n",
        "        self.api_key = \"lwhfs47ilbvwc84g\"\n",
        "        self.access_token = \"ozWrM7XdI0CUbzzFiZQwaUfpSmqkblXY\"\n",
        "\n",
        "        # Initialize KiteConnect\n",
        "        self.kite = KiteConnect(api_key=self.api_key)\n",
        "        self.kite.set_access_token(self.access_token)\n",
        "\n",
        "    def fetch_equal_weighted_returns_monthly(self, symbols, month):\n",
        "        \"\"\"\n",
        "        Fetch monthly candle data for the given symbols and calculate equal-weighted returns.\n",
        "        Fallback to vbt.TVData if data is not available on Zerodha.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Fetching equal-weighted returns for month: {month}\")\n",
        "            monthly_returns = []\n",
        "\n",
        "            if not symbols:\n",
        "                self.logger.warning(f\"No symbols provided for month: {month}\")\n",
        "                return np.nan\n",
        "\n",
        "            # Convert month to datetime objects\n",
        "            current_month_start = pd.Timestamp(f\"{month}-01\", tz=\"Asia/Kolkata\")\n",
        "            previous_month_start = current_month_start - pd.offsets.MonthBegin(1)\n",
        "            current_month_end = current_month_start + pd.offsets.MonthEnd(0)\n",
        "\n",
        "            for symbol in symbols:\n",
        "                try:\n",
        "                    self.logger.info(f\"Pulling data for symbol: {symbol}\")\n",
        "                    instrument_token = self.get_instrument_token(symbol.replace(\"NSE:\", \"\"))\n",
        "\n",
        "                    if instrument_token:\n",
        "                        # Fetch data using Zerodha API\n",
        "                        data = self.kite.historical_data(\n",
        "                            instrument_token=instrument_token,\n",
        "                            from_date=previous_month_start.to_pydatetime(),\n",
        "                            to_date=current_month_end.to_pydatetime(),\n",
        "                            interval=\"day\"\n",
        "                        )\n",
        "\n",
        "                        # Convert data to DataFrame\n",
        "                        df = pd.DataFrame(data)\n",
        "                        if df.empty:\n",
        "                            self.logger.warning(f\"No data available for symbol: {symbol}\")\n",
        "                            continue\n",
        "\n",
        "                        df['date'] = pd.to_datetime(df['date'])\n",
        "                        if df['date'].dt.tz is None:\n",
        "                            df['date'] = df['date'].dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
        "                        else:\n",
        "                            df['date'] = df['date'].dt.tz_convert('Asia/Kolkata')\n",
        "\n",
        "                        df.set_index('date', inplace=True)\n",
        "\n",
        "                        # Get Close prices for the current and previous months\n",
        "                        previous_month_close = df.loc[df.index.to_period('M') == previous_month_start.to_period('M'), 'close']\n",
        "                        current_month_close = df.loc[df.index.to_period('M') == current_month_start.to_period('M'), 'close']\n",
        "\n",
        "                    else:\n",
        "                        # Fallback to vbt.TVData if no instrument token is found\n",
        "                        self.logger.warning(f\"Instrument token not found for symbol: {symbol}. Using vbt.TVData.\")\n",
        "                        data = vbt.TVData.pull(symbol, timeframe=\"1M\", tz=\"GMT+5:30\")\n",
        "                        close = data.get(\"Close\")\n",
        "\n",
        "                        if close.index.tz is None:\n",
        "                            close = close.tz_localize(\"UTC\").tz_convert(\"Asia/Kolkata\")\n",
        "                        else:\n",
        "                            close = close.tz_convert(\"Asia/Kolkata\")\n",
        "\n",
        "                        previous_month_close = close.loc[close.index.to_period('M') == previous_month_start.to_period('M')]\n",
        "                        current_month_close = close.loc[close.index.to_period('M') == current_month_start.to_period('M')]\n",
        "\n",
        "                    if not previous_month_close.empty and not current_month_close.empty:\n",
        "                        return_value = current_month_close.iloc[-1] / previous_month_close.iloc[-1] - 1\n",
        "                        monthly_returns.append(return_value)\n",
        "                        self.logger.info(f\"Return for symbol {symbol}: {return_value:.4f}\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Skipping symbol due to insufficient data: {symbol}, month: {month}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error fetching data for symbol: {symbol}, error: {e}\")\n",
        "\n",
        "            # Calculate equal-weighted return\n",
        "            if monthly_returns:\n",
        "                result = round(np.mean(monthly_returns), 4)\n",
        "                self.logger.info(f\"Equal-weighted return for month {month}: {result}\")\n",
        "                return result\n",
        "            else:\n",
        "                self.logger.warning(f\"No valid returns calculated for month: {month}\")\n",
        "                return np.nan\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching equal-weighted returns for month: {month}, error: {e}\")\n",
        "            return np.nan\n",
        "\n",
        "    def fetch_and_save_instruments(self, file_path=\"instruments.csv\"):\n",
        "        \"\"\"\n",
        "        Fetch all instruments from Zerodha and save them locally as a CSV file.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Fetching all instruments from Zerodha...\")\n",
        "            instruments = self.kite.instruments(\"NSE\")\n",
        "            instruments_df = pd.DataFrame(instruments)\n",
        "            instruments_df.to_csv(file_path, index=False)\n",
        "            self.logger.info(f\"Fetched and saved {len(instruments_df)} instruments to {file_path}.\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching instruments: {e}\")\n",
        "\n",
        "    def load_instruments(self, file_path=\"instruments.csv\"):\n",
        "        \"\"\"\n",
        "        Load instruments from a local CSV file into a DataFrame.\n",
        "        Fetch from Zerodha and save locally if the file does not exist.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Loading instruments from {file_path}...\")\n",
        "            instruments_df = pd.read_csv(file_path)\n",
        "            self.instrument_lookup = pd.Series(\n",
        "                instruments_df[\"instrument_token\"].values, index=instruments_df[\"tradingsymbol\"]\n",
        "            ).to_dict()\n",
        "            self.logger.info(f\"Loaded {len(self.instrument_lookup)} instruments from {file_path}.\")\n",
        "        except FileNotFoundError:\n",
        "            self.logger.warning(f\"{file_path} not found. Fetching from Zerodha...\")\n",
        "            self.fetch_and_save_instruments(file_path)\n",
        "            self.load_instruments(file_path)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading instruments from {file_path}: {e}\")\n",
        "            self.instrument_lookup = {}\n",
        "\n",
        "\n",
        "\n",
        "    def get_instrument_token(self, symbol):\n",
        "        \"\"\"\n",
        "        Retrieve the instrument token for a given symbol from the cached instruments.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Remove the 'NSE:' prefix if it exists\n",
        "            clean_symbol = symbol.replace(\"NSE:\", \"\")\n",
        "\n",
        "            if not hasattr(self, 'instrument_lookup') or not self.instrument_lookup:\n",
        "                self.load_instruments()\n",
        "\n",
        "            instrument_token = self.instrument_lookup.get(clean_symbol)\n",
        "            if instrument_token:\n",
        "                return instrument_token\n",
        "            else:\n",
        "                self.logger.warning(f\"Instrument token not found for symbol: {symbol}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching instrument token for symbol: {symbol}, error: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def fetch_cnxsmallcap_returns(self):\n",
        "        \"\"\"\n",
        "        Fetch monthly adjusted close prices for CNXSMALLCAP using Zerodha API and calculate monthly returns.\n",
        "        Fallback to vbt.TVData if data is not available on Zerodha.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Fetching CNXSMALLCAP returns...\")\n",
        "            instrument_token = self.get_instrument_token(\"CNXSMALLCAP\")\n",
        "            if not instrument_token:\n",
        "                self.logger.warning(\"Instrument token not found for CNXSMALLCAP. Using vbt.TVData fallback.\")\n",
        "                # Fallback to vbt.TVData\n",
        "                data = vbt.TVData.pull(\"NSE:CNXSMALLCAP\", timeframe=\"1M\", tz=\"GMT+5:30\")\n",
        "                close = data.get(\"Close\")\n",
        "\n",
        "                # Ensure timezone-awareness\n",
        "                if close.index.tz is None:\n",
        "                    close = close.tz_localize(\"UTC\").tz_convert(\"Asia/Kolkata\")\n",
        "                else:\n",
        "                    close = close.tz_convert(\"Asia/Kolkata\")\n",
        "\n",
        "                # Calculate monthly returns\n",
        "                cnxsmallcap_returns = close.pct_change().rename(\"CNXSMALLCAP_Returns\").to_frame()\n",
        "                cnxsmallcap_returns['Month'] = cnxsmallcap_returns.index.to_period('M')\n",
        "                self.logger.info(\"CNXSMALLCAP returns fetched using vbt.TVData.\")\n",
        "                return cnxsmallcap_returns\n",
        "\n",
        "            # Fetch data using Zerodha API\n",
        "            data = self.kite.historical_data(\n",
        "                instrument_token=instrument_token,\n",
        "                from_date=(datetime.now() - pd.DateOffset(years=5)).to_pydatetime(),\n",
        "                to_date=datetime.now(),\n",
        "                interval=\"month\"\n",
        "            )\n",
        "\n",
        "            # Convert data to DataFrame\n",
        "            df = pd.DataFrame(data)\n",
        "            if df.empty:\n",
        "                self.logger.warning(\"No data available for CNXSMALLCAP.\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df['date'] = pd.to_datetime(df['date']).dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
        "            df.set_index('date', inplace=True)\n",
        "\n",
        "            # Calculate monthly returns\n",
        "            cnxsmallcap_returns = df['close'].pct_change().rename(\"CNXSMALLCAP_Returns\").to_frame()\n",
        "            cnxsmallcap_returns['Month'] = cnxsmallcap_returns.index.to_period('M')\n",
        "            self.logger.info(\"CNXSMALLCAP returns fetched successfully.\")\n",
        "            return cnxsmallcap_returns\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching CNXSMALLCAP data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "    def generate_summary(self, tradebook_df, symbols_data, cnxsmallcap_returns):\n",
        "        \"\"\"\n",
        "        Generate a summary of trade metrics and merge with CNXSMALLCAP returns and cumulative equal-weighted returns.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Generating summary from the tradebook...\")\n",
        "\n",
        "            # Ensure 'Month' column exists\n",
        "            if 'Month' not in tradebook_df.columns:\n",
        "                if 'Entry Index' in tradebook_df.columns:\n",
        "                    self.logger.info(\"Creating 'Month' column from 'Entry Index'.\")\n",
        "                    tradebook_df['Entry Index'] = pd.to_datetime(tradebook_df['Entry Index'], errors='coerce')\n",
        "                    tradebook_df['Month'] = tradebook_df['Entry Index'].dt.to_period('M')  # Extract month as Period[M]\n",
        "                else:\n",
        "                    self.logger.error(\"Missing required 'Entry Index' column to create 'Month'.\")\n",
        "                    raise KeyError(\"The 'Month' column is missing, and 'Entry Index' is not available to create it.\")\n",
        "\n",
        "            # Group by 'Month' and calculate metrics\n",
        "            summary = tradebook_df.groupby('Month').apply(self.calculate_metrics).reset_index()\n",
        "\n",
        "            # Add number of scrips from the symbols.json\n",
        "            summary['Num_Scrips_Input'] = summary['Month'].apply(\n",
        "                lambda x: len(symbols_data.get(str(x) + \"-01\", {}))\n",
        "            )\n",
        "\n",
        "            # Add strategy return early\n",
        "            summary['Strategy_Return'] = round(summary['Month_PnL'] / 1_000_000, 4)\n",
        "\n",
        "            # Calculate equal-weighted returns (without cost adjustment)\n",
        "            summary['Equal_Weighted_Return'] = summary['Month'].apply(\n",
        "                lambda month: self.fetch_equal_weighted_returns_monthly(\n",
        "                    symbols_data.get(str(month) + \"-01\", {}).keys(), str(month)\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Calculate equal-weighted returns (cost-adjusted by 0.4%)\n",
        "            summary['Equal_Weighted_Return_Cost_Adjusted'] = summary['Equal_Weighted_Return'] - 0.004\n",
        "\n",
        "            # Calculate cumulative equal-weighted returns\n",
        "            summary['Cumulative_Equal_Weighted_Return'] = round(\n",
        "                (1 + summary['Equal_Weighted_Return']).cumprod() - 1, 4\n",
        "            )\n",
        "\n",
        "            # Calculate cumulative equal-weighted returns (cost-adjusted by 0.4%)\n",
        "            summary['Cumulative_Equal_Weighted_Return_Cost_Adjusted'] = round(\n",
        "                (1 + summary['Equal_Weighted_Return_Cost_Adjusted']).cumprod() - 1, 4\n",
        "            )\n",
        "\n",
        "            # Add cumulative returns with reinvestments\n",
        "            summary['Strategy_Cumulative_Return_With_Reinvestments'] = round(\n",
        "                (1 + summary['Strategy_Return']).cumprod() - 1, 4\n",
        "            )\n",
        "\n",
        "            # Merge the CNXSMALLCAP monthly returns into the summary\n",
        "            summary = summary.merge(cnxsmallcap_returns, on='Month', how='left')\n",
        "\n",
        "            # Calculate cumulative returns for both CNXSMALLCAP and the strategy\n",
        "            summary['Cumulative_Strategy_Return'] = round((1 + summary['Strategy_Return']).cumprod() - 1, 4)\n",
        "            summary['Cumulative_CNXSMALLCAP_Return'] = round((1 + summary['CNXSMALLCAP_Returns']).cumprod() - 1, 4)\n",
        "\n",
        "            # Calculate drawdowns\n",
        "            summary['Max_Cumulative_Strategy_Return'] = summary['Cumulative_Strategy_Return'].cummax()\n",
        "            summary['Strategy_Drawdown'] = round(\n",
        "                (summary['Cumulative_Strategy_Return'] - summary['Max_Cumulative_Strategy_Return'])\n",
        "                / summary['Max_Cumulative_Strategy_Return'], 4\n",
        "            )\n",
        "\n",
        "            summary['Max_Cumulative_CNXSMALLCAP_Return'] = summary['Cumulative_CNXSMALLCAP_Return'].cummax()\n",
        "            summary['CNXSMALLCAP_Drawdown'] = round(\n",
        "                (summary['Cumulative_CNXSMALLCAP_Return'] - summary['Max_Cumulative_CNXSMALLCAP_Return'])\n",
        "                / summary['Max_Cumulative_CNXSMALLCAP_Return'], 4\n",
        "            )\n",
        "\n",
        "            # Drop unnecessary columns\n",
        "            summary = summary.drop(columns=[\n",
        "                'Max_Cumulative_Strategy_Return',\n",
        "                'Max_Cumulative_CNXSMALLCAP_Return'\n",
        "            ], errors='ignore')\n",
        "\n",
        "            self.logger.info(\"Summary generated successfully.\")\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating summary: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "    def calculate_metrics(self, group):\n",
        "        \"\"\"\n",
        "        Calculate additional metrics for the tradebook summary.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Calculating metrics for a trade group...\")\n",
        "        winning_trades = group[group['PnL'] > 0]\n",
        "        losing_trades = group[group['PnL'] < 0]\n",
        "\n",
        "        metrics = {\n",
        "            'Month_PnL': round(group['PnL'].sum(), 2),\n",
        "            'Month_Trades': len(group),\n",
        "            'Unique_Scrips': group['Column'].nunique(),\n",
        "            'Max_Loss': round(group['PnL'].min(), 2),\n",
        "            'Max_Profit': round(group['PnL'].max(), 2),\n",
        "            'Median_Profit': round(winning_trades['PnL'].median(), 2) if not winning_trades.empty else 0,\n",
        "            'Median_Loss': round(losing_trades['PnL'].median(), 2) if not losing_trades.empty else 0,\n",
        "            'Win_Rate': round(len(winning_trades) / len(group), 2) if len(group) > 0 else 0,\n",
        "            'Profit_Factor': round(winning_trades['PnL'].sum() / abs(losing_trades['PnL'].sum()), 2) if not losing_trades.empty else float('inf'),\n",
        "            'R_Multiple': round((winning_trades['Return'].mean() / abs(losing_trades['Return'].mean())), 2) if not losing_trades.empty else float('inf'),\n",
        "        }\n",
        "        self.logger.info(f\"Metrics calculated: {metrics}\")\n",
        "        return pd.Series(metrics)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kiGEIcBa6bJT"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "eDq9TvmPqZSC",
        "outputId": "5ef134fe-75b6-43bd-d550-dd26d3e531a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-12-30T17:34:49.824398+05:30 - INFO - KiteConnect initialized successfully.\n",
            "KiteConnect initialized successfully.\n",
            "KiteConnect initialized successfully.\n",
            "KiteConnect initialized successfully.\n",
            "INFO:OrderBookGeneratorLogger:KiteConnect initialized successfully.\n",
            "2024-12-30T17:34:49.849334+05:30 - INFO - Loaded symbols JSON from /content/symbols.json\n",
            "Loaded symbols JSON from /content/symbols.json\n",
            "Loaded symbols JSON from /content/symbols.json\n",
            "Loaded symbols JSON from /content/symbols.json\n",
            "INFO:OrderBookGeneratorLogger:Loaded symbols JSON from /content/symbols.json\n",
            "2024-12-30T17:34:49.863140+05:30 - INFO - Processing data for the target month: 2024-12-01\n",
            "Processing data for the target month: 2024-12-01\n",
            "Processing data for the target month: 2024-12-01\n",
            "Processing data for the target month: 2024-12-01\n",
            "INFO:OrderBookGeneratorLogger:Processing data for the target month: 2024-12-01\n",
            "2024-12-30T17:34:49.890007+05:30 - INFO - Fetching NSE instruments from KiteConnect...\n",
            "Fetching NSE instruments from KiteConnect...\n",
            "Fetching NSE instruments from KiteConnect...\n",
            "Fetching NSE instruments from KiteConnect...\n",
            "INFO:OrderBookGeneratorLogger:Fetching NSE instruments from KiteConnect...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-b3a9639f6a99>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Instantiate the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m OBGenerator = OrderBookGenerator(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlog_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_file_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-dee9b9e12dfd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, api_key, access_token, log_file, symbols_json_path, default_cash, tp_ladder, mult_factor, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Fetch instrument data from Kite API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstrument_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_instruments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-dee9b9e12dfd>\u001b[0m in \u001b[0;36mfetch_instruments\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fetching NSE instruments from KiteConnect...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0minstrument_dump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NSE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0minstrument_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstrument_dump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0minstrument_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NSE_Instruments.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save as CSV for reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kiteconnect/connect.py\u001b[0m in \u001b[0;36minstruments\u001b[0;34m(self, exchange)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \"\"\"\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexchange\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_instruments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"market.instruments\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"exchange\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexchange\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_instruments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"market.instruments.all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kiteconnect/connect.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, route, url_args, params, is_json)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;34m\"\"\"Alias for sending a GET request.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kiteconnect/connect.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, route, method, url_args, params, is_json, query_params)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             r = self.reqsession.request(method,\n\u001b[0m\u001b[1;32m    905\u001b[0m                                         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                                         \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_json\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    947\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def _raw_read(\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "log_file_path = f\"logfile_{datetime.now(IST).strftime('%Y-%m-%d')}.log\"\n",
        "\n",
        "# Setup logger\n",
        "logger = logging.getLogger(\"OrderBookGeneratorLogger\")\n",
        "logger.setLevel(logging.INFO)\n",
        "console_handler = logging.StreamHandler()\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the class\n",
        "OBGenerator = OrderBookGenerator(\n",
        "    logger=logger,\n",
        "    log_file=log_file_path,\n",
        "    symbols_json_path='/content/symbols.json',\n",
        "    api_key = api_key,\n",
        "    access_token=access_token\n",
        ")\n",
        "\n",
        "# Test if get_orderbook is recognized\n",
        "if hasattr(OBGenerator, 'get_orderbook'):\n",
        "    print(\"get_orderbook method exists!\")\n",
        "else:\n",
        "    print(\"get_orderbook method is missing!\")\n",
        "\n",
        "# Generate the order book\n",
        "try:\n",
        "    df = OBGenerator.get_orderbook(max_months=max_month)\n",
        "    print(\"Order book generated successfully!\")\n",
        "    print(df.head())\n",
        "except AttributeError as e:\n",
        "    print(f\"AttributeError: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error: {e}\")\n",
        "\n",
        "\n",
        "# Ensure datetime columns are properly formatted\n",
        "df[\"Entry Index\"] = pd.to_datetime(df[\"Entry Index\"])\n",
        "df[\"Exit Index\"] = pd.to_datetime(df[\"Exit Index\"], errors='coerce')  # Allow for NaT for open trades\n",
        "\n",
        "# The final DataFrame `df` contains the complete order book for the latest 12 months\n",
        "df.to_csv('tradebook.csv')\n",
        "\n",
        "\n",
        "# Create an instance of the class\n",
        "summary_generator = TradeSummaryGenerator(logger)\n",
        "\n",
        "# Example inputs\n",
        "tradebook_df = pd.read_csv(\"tradebook.csv\")  # Load the tradebook data\n",
        "symbols_data = json.load(open(\"symbols.json\"))  # Load the symbols.json\n",
        "cnxsmallcap_returns = summary_generator.fetch_cnxsmallcap_returns()\n",
        "\n",
        "# Generate the summary\n",
        "summary = summary_generator.generate_summary(tradebook_df, symbols_data, cnxsmallcap_returns)\n",
        "\n",
        "# Save the summary to a CSV\n",
        "summary.to_csv(\"summary.csv\", index=False)\n",
        "logger.info(\"Summary saved to summary.csv.\")\n",
        "\n",
        "\n",
        "\n",
        "summary\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKWaBlPVcLzLmae6SybanR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}